{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "import datetime\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "from shared_lib import utils, vocabulary\n",
    "from shared_lib import ngram_lm\n",
    "from shared_lib import ngram_utils\n",
    "from shared_lib import simple_trigram\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "from collections import OrderedDict\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston_bomb_df = pd.read_json('/Users/krista/Desktop/w266-project-master/data/reddit/boston_comments_2013.03-2013.05.txt', lines=True)\n",
    "boston_series_df = pd.read_json('/Users/krista/Desktop/w266-project-master/data/reddit/boston_comments_2013.10-2013.11.txt', lines=True)\n",
    "colorado_df = pd.read_json('/Users/krista/Desktop/w266-project-master/data/reddit/colorado_comments_2017.06-2017.09.txt', lines=True)\n",
    "florida_df = pd.read_json('/Users/krista/Desktop/w266-project-master/data/reddit/florida_comments_2017.06-2017.09.txt', lines=True)\n",
    "houston_df = pd.read_json('/Users/krista/Desktop/w266-project-master/data/reddit/houston_comments_2017.06-2017.09.txt', lines=True)\n",
    "miami_df = pd.read_json('/Users/krista/Desktop/w266-project-master/data/reddit/miami_comments_2017.06-2017.09.txt', lines=True)\n",
    "nyc_df = pd.read_json('/Users/krista/Desktop/w266-project-master/data/reddit/nyc_comments_2012.08-2012.12.txt', lines=True)\n",
    "puerto_rico_df = pd.read_json('/Users/krista/Desktop/w266-project-master/data/reddit/puerto_rico_comments_2017.06-2017.09.txt', lines=True)\n",
    "vegas_df = pd.read_json('/Users/krista/Desktop/w266-project-master/data/reddit/vegas_comments_2017.06-2017.09.txt', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup local times\n",
    "boston_bomb_df['created_at_local'] = pd.to_datetime(boston_bomb_df['created_utc'], unit='s') \\\n",
    "                                     .dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
    "boston_series_df['created_at_local'] = pd.to_datetime(boston_series_df['created_utc'], unit='s') \\\n",
    "                                     .dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
    "colorado_df['created_at_local'] = pd.to_datetime(colorado_df['created_utc'], unit='s') \\\n",
    "                                     .dt.tz_localize('UTC').dt.tz_convert('US/Mountain')\n",
    "florida_df['created_at_local'] = pd.to_datetime(florida_df['created_utc'], unit='s') \\\n",
    "                                     .dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
    "houston_df['created_at_local'] = pd.to_datetime(houston_df['created_utc'], unit='s') \\\n",
    "                                     .dt.tz_localize('UTC').dt.tz_convert('US/Central')\n",
    "miami_df['created_at_local'] = pd.to_datetime(miami_df['created_utc'], unit='s') \\\n",
    "                                     .dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
    "nyc_df['created_at_local'] = pd.to_datetime(nyc_df['created_utc'], unit='s') \\\n",
    "                                 .dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
    "puerto_rico_df['created_at_local'] = pd.to_datetime(puerto_rico_df['created_utc'], unit='s') \\\n",
    "                                 .dt.tz_localize('UTC').dt.tz_convert('America/Puerto_Rico')\n",
    "vegas_df['created_at_local'] = pd.to_datetime(vegas_df['created_utc'], unit='s') \\\n",
    "                                 .dt.tz_localize('UTC').dt.tz_convert('US/Pacific')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def get_raw_text(df):\n",
    "    raw_text = ''\n",
    "    for post in df['body']:\n",
    "        raw_text = ' '.join([raw_text, post])\n",
    "    return raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#boston_series_df.sort_values('created_at_local', ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sanity check for my stopword removal\n",
    "# foo = ['foo']\n",
    "\n",
    "# bar = ['bar', 'foo']\n",
    "\n",
    "# man = [i for i in bar if i not in foo]\n",
    "\n",
    "# print man"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hurricane Irma (Florida/Miami), August 30th - Sept. 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'gt', u'the', u'Tri', u'Rail', u'doesn']\n"
     ]
    }
   ],
   "source": [
    "#tokenize words\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "tokens = tokenizer.tokenize(get_raw_text(miami_df))\n",
    "\n",
    "print tokens[:5]\n",
    "token_feed = (utils.canonicalize_word(w) for w in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 23761\n",
      "Most common unigrams:\n",
      "\"the\": 23230\n",
      "\"to\": 16458\n",
      "\"i\": 14764\n",
      "\"a\": 14188\n",
      "\"and\": 13497\n",
      "\"you\": 10493\n",
      "\"it\": 10006\n",
      "\"of\": 9656\n",
      "\"in\": 9652\n",
      "\"is\": 8561\n",
      "\"that\": 7640\n",
      "\"for\": 6470\n",
      "\"s\": 6322\n",
      "\"t\": 5738\n",
      "\"on\": 5071\n",
      "\"they\": 4623\n",
      "\"are\": 4577\n",
      "\"have\": 4519\n",
      "\"be\": 4315\n",
      "\"but\": 4235\n"
     ]
    }
   ],
   "source": [
    "# Collect counts of tokens and assign wordids.\n",
    "vocab = vocabulary.Vocabulary(token_feed)\n",
    "print \"Vocabulary size: %d\" % vocab.size\n",
    "\n",
    "# Print out some (debugging) statistics to make sure everything went\n",
    "# as we expected.  (Unsurprisingly, you should see \"the\" as the most popular word.)\n",
    "print \"Most common unigrams:\"\n",
    "for word, count in vocab.unigram_counts.most_common(20):\n",
    "    print \"\\\"%s\\\": %d\" % (word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove single letters\n",
    "\n",
    "tokens =[i for i in tokens if len(i)>1]\n",
    "\n",
    "#lowercase everything\n",
    "\n",
    "tokens = [ i.lower() for i in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'gt', u'tri', u'rail', u'connect', u'two']\n"
     ]
    }
   ],
   "source": [
    "from stop_words import get_stop_words\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# remove stop words from tokens from python stop_tokens\n",
    "stopped_tokens = [i for i in tokens if i not in en_stop]\n",
    "\n",
    "#remove stop words from nltk stop_tokens\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "#stopped_tokens = [i for i in tokens if i not in stop_words]\n",
    "\n",
    "remove_phrases =[\"'s\",\"n't\", \"'m\",\"''\", \"''\", \"The\", \"https\"]\n",
    "\n",
    "#stopped_tokens = [ i for i in tokens if not in remove_phrases]\n",
    "\n",
    "print stopped_tokens[:5]\n",
    "\n",
    "texts= stopped_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 24465\n",
      "Most common unigrams:\n",
      "\"miami\": 2939\n",
      "\"people\": 2588\n",
      "\"get\": 2228\n",
      "\"like\": 2222\n",
      "\"com\": 1749\n",
      "\"power\": 1732\n",
      "\"one\": 1685\n",
      "\"would\": 1588\n",
      "\"deleted\": 1481\n",
      "\"good\": 1362\n",
      "\"https\": 1347\n",
      "\"go\": 1343\n",
      "\"know\": 1322\n",
      "\"amp\": 1254\n",
      "\"still\": 1194\n",
      "\"going\": 1165\n",
      "\"back\": 1146\n",
      "\"also\": 1104\n",
      "\"time\": 1097\n",
      "\"think\": 1090\n"
     ]
    }
   ],
   "source": [
    "# Collect counts of tokens and assign wordids.\n",
    "vocab = vocabulary.Vocabulary(stopped_tokens)\n",
    "print \"Vocabulary size: %d\" % vocab.size\n",
    "\n",
    "# Print out some (debugging) statistics to make sure everything went\n",
    "# as we expected.  (Unsurprisingly, you should see \"the\" as the most popular word.)\n",
    "print \"Most common unigrams:\"\n",
    "for word, count in vocab.unigram_counts.most_common(20):\n",
    "    print \"\\\"%s\\\": %d\" % (word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some papers claimed stemming doesn't help\n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "#p_stemmer = PorterStemmer()\n",
    "\n",
    "# stem token\n",
    "#texts = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "\n",
    "# from pprint import pprint\n",
    "\n",
    "# pprint(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "[u'gt' u'tri' u'rail' u'connect' u'two']\n"
     ]
    }
   ],
   "source": [
    "#convert to an array\n",
    "texts_array = np.asarray(texts)\n",
    "print type(texts_array)\n",
    "\n",
    "print texts_array[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 24465\n",
      "Most common unigrams:\n",
      "\"miami\": 2939\n",
      "\"people\": 2588\n",
      "\"get\": 2228\n",
      "\"like\": 2222\n",
      "\"com\": 1749\n",
      "\"power\": 1732\n",
      "\"one\": 1685\n",
      "\"would\": 1588\n",
      "\"deleted\": 1481\n",
      "\"good\": 1362\n",
      "\"https\": 1347\n",
      "\"go\": 1343\n",
      "\"know\": 1322\n",
      "\"amp\": 1254\n",
      "\"still\": 1194\n",
      "\"going\": 1165\n",
      "\"back\": 1146\n",
      "\"also\": 1104\n",
      "\"time\": 1097\n",
      "\"think\": 1090\n"
     ]
    }
   ],
   "source": [
    "# Collect counts of tokens and assign wordids.\n",
    "vocab = vocabulary.Vocabulary(texts)\n",
    "print \"Vocabulary size: %d\" % vocab.size\n",
    "\n",
    "# Print out some (debugging) statistics to make sure everything went\n",
    "# as we expected.  (Unsurprisingly, you should see \"the\" as the most popular word.)\n",
    "print \"Most common unigrams:\"\n",
    "for word, count in vocab.unigram_counts.most_common(20):\n",
    "    print \"\\\"%s\\\": %d\" % (word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#texts_array_irma = texts_arr\n",
    "\n",
    "texts_irma=texts\n",
    "\n",
    "texts_2= [u'hi']\n",
    "\n",
    "texts_all=[texts_irma, texts_2]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts_all)\n",
    "\n",
    "#print (texts[:5])\n",
    "#doct_stream = (tokens for _, tokens in stopped_tokens)\n",
    "#print texts_all[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(42, u'0.000*\"people\" + 0.000*\"miami\" + 0.000*\"like\" + 0.000*\"get\" + 0.000*\"com\" + 0.000*\"deleted\" + 0.000*\"would\" + 0.000*\"power\" + 0.000*\"one\" + 0.000*\"good\"'), (29, u'0.000*\"miami\" + 0.000*\"get\" + 0.000*\"people\" + 0.000*\"one\" + 0.000*\"com\" + 0.000*\"deleted\" + 0.000*\"like\" + 0.000*\"https\" + 0.000*\"think\" + 0.000*\"would\"'), (31, u'0.000*\"miami\" + 0.000*\"like\" + 0.000*\"people\" + 0.000*\"get\" + 0.000*\"one\" + 0.000*\"deleted\" + 0.000*\"com\" + 0.000*\"power\" + 0.000*\"would\" + 0.000*\"https\"'), (73, u'0.007*\"souls\" + 0.004*\"boos\" + 0.004*\"bc0k8qyr\" + 0.004*\"413f\" + 0.004*\"coffeeshops\" + 0.004*\"antarctica\" + 0.004*\"0565732\" + 0.004*\"floodlights\" + 0.004*\"debunked\" + 0.004*\"paragraphs\"'), (32, u'0.009*\"miami\" + 0.008*\"people\" + 0.007*\"like\" + 0.006*\"get\" + 0.005*\"com\" + 0.005*\"one\" + 0.005*\"would\" + 0.005*\"power\" + 0.004*\"https\" + 0.004*\"go\"'), (96, u'0.000*\"miami\" + 0.000*\"like\" + 0.000*\"people\" + 0.000*\"com\" + 0.000*\"deleted\" + 0.000*\"go\" + 0.000*\"would\" + 0.000*\"one\" + 0.000*\"power\" + 0.000*\"get\"'), (41, u'0.000*\"miami\" + 0.000*\"people\" + 0.000*\"get\" + 0.000*\"like\" + 0.000*\"com\" + 0.000*\"power\" + 0.000*\"deleted\" + 0.000*\"one\" + 0.000*\"amp\" + 0.000*\"go\"'), (17, u'0.000*\"miami\" + 0.000*\"get\" + 0.000*\"people\" + 0.000*\"like\" + 0.000*\"power\" + 0.000*\"one\" + 0.000*\"also\" + 0.000*\"https\" + 0.000*\"would\" + 0.000*\"good\"'), (16, u'0.000*\"miami\" + 0.000*\"deleted\" + 0.000*\"like\" + 0.000*\"would\" + 0.000*\"people\" + 0.000*\"power\" + 0.000*\"still\" + 0.000*\"get\" + 0.000*\"https\" + 0.000*\"com\"'), (22, u'0.000*\"miami\" + 0.000*\"get\" + 0.000*\"like\" + 0.000*\"people\" + 0.000*\"one\" + 0.000*\"deleted\" + 0.000*\"power\" + 0.000*\"com\" + 0.000*\"would\" + 0.000*\"go\"')]\n"
     ]
    }
   ],
   "source": [
    "ldamodel_100 = gensim.models.ldamodel.LdaModel(corpus, num_topics=100, id2word= dictionary, passes= 20)\n",
    "print(ldamodel_100.print_topics(num_topics=10, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15, u'0.007*\"miami\" + 0.007*\"people\" + 0.007*\"like\" + 0.007*\"get\" + 0.006*\"power\" + 0.005*\"com\" + 0.005*\"one\" + 0.004*\"know\" + 0.004*\"go\" + 0.004*\"still\"'), (14, u'0.008*\"people\" + 0.008*\"get\" + 0.007*\"miami\" + 0.005*\"like\" + 0.005*\"power\" + 0.005*\"amp\" + 0.004*\"think\" + 0.004*\"one\" + 0.004*\"com\" + 0.004*\"would\"'), (25, u'0.008*\"miami\" + 0.007*\"people\" + 0.006*\"like\" + 0.006*\"one\" + 0.006*\"com\" + 0.006*\"get\" + 0.005*\"deleted\" + 0.004*\"power\" + 0.004*\"know\" + 0.004*\"good\"'), (16, u'0.008*\"miami\" + 0.008*\"people\" + 0.005*\"power\" + 0.005*\"one\" + 0.005*\"get\" + 0.004*\"would\" + 0.004*\"https\" + 0.004*\"know\" + 0.004*\"also\" + 0.004*\"still\"'), (37, u'0.009*\"miami\" + 0.008*\"people\" + 0.006*\"power\" + 0.006*\"get\" + 0.006*\"https\" + 0.005*\"deleted\" + 0.004*\"go\" + 0.004*\"going\" + 0.004*\"like\" + 0.004*\"one\"'), (32, u'0.010*\"miami\" + 0.008*\"people\" + 0.006*\"com\" + 0.006*\"would\" + 0.006*\"like\" + 0.005*\"one\" + 0.005*\"get\" + 0.004*\"amp\" + 0.004*\"deleted\" + 0.004*\"good\"'), (38, u'0.007*\"get\" + 0.007*\"miami\" + 0.007*\"like\" + 0.005*\"com\" + 0.005*\"people\" + 0.005*\"https\" + 0.005*\"one\" + 0.005*\"know\" + 0.005*\"amp\" + 0.004*\"power\"'), (26, u'0.009*\"miami\" + 0.008*\"get\" + 0.007*\"people\" + 0.006*\"one\" + 0.005*\"power\" + 0.005*\"com\" + 0.005*\"like\" + 0.004*\"deleted\" + 0.004*\"good\" + 0.004*\"going\"'), (47, u'0.009*\"people\" + 0.009*\"miami\" + 0.008*\"like\" + 0.007*\"get\" + 0.005*\"one\" + 0.005*\"deleted\" + 0.004*\"com\" + 0.004*\"know\" + 0.004*\"good\" + 0.004*\"back\"'), (0, u'0.008*\"miami\" + 0.006*\"people\" + 0.006*\"get\" + 0.006*\"like\" + 0.005*\"https\" + 0.005*\"power\" + 0.005*\"com\" + 0.004*\"would\" + 0.004*\"time\" + 0.004*\"good\"')]\n"
     ]
    }
   ],
   "source": [
    "ldamodel_50 = gensim.models.ldamodel.LdaModel(corpus, num_topics=50, id2word= dictionary)\n",
    "print(ldamodel_50.print_topics(num_topics=10, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# doc_stream = (tokens for _, tokens in token_feed)  # generator\n",
    "# test_docs = list(itertools.islice(doc_stream, 8000, 9000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def intra_inter(model, test_docs, num_pairs=10000):\n",
    "#     # split each test document into two halves and compute topics for each half\n",
    "#     part1 = [model[dictionary.doc2bow(tokens[: len(tokens) / 2])] for tokens in test_docs]\n",
    "#     part2 = [model[dictionary.doc2bow(tokens[len(tokens) / 2 :])] for tokens in test_docs]\n",
    "    \n",
    "#     # print computed similarities (uses cossim)\n",
    "#     print(\"average cosine similarity between corresponding parts (higher is better):\")\n",
    "#     print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part1, part2)]))\n",
    "\n",
    "#     random_pairs = np.random.randint(0, len(test_docs), size=(num_pairs, 2))\n",
    "#     print(\"average cosine similarity between 10,000 random parts (lower is better):\")    \n",
    "#     print(np.mean([gensim.matutils.cossim(part1[i[0]], part2[i[1]]) for i in random_pairs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(\"LDA results:\")\n",
    "# intra_inter(ldamodel_25, test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(17, u'0.010*\"miami\" + 0.008*\"like\" + 0.007*\"people\" + 0.006*\"get\" + 0.006*\"com\" + 0.005*\"power\" + 0.005*\"one\" + 0.004*\"would\" + 0.004*\"good\" + 0.004*\"back\"'), (15, u'0.008*\"get\" + 0.007*\"like\" + 0.006*\"people\" + 0.005*\"power\" + 0.004*\"one\" + 0.004*\"miami\" + 0.004*\"com\" + 0.004*\"back\" + 0.004*\"deleted\" + 0.004*\"also\"'), (14, u'0.009*\"like\" + 0.008*\"miami\" + 0.007*\"people\" + 0.005*\"get\" + 0.005*\"one\" + 0.005*\"com\" + 0.004*\"know\" + 0.004*\"time\" + 0.004*\"power\" + 0.004*\"deleted\"'), (11, u'0.008*\"get\" + 0.007*\"miami\" + 0.007*\"people\" + 0.007*\"like\" + 0.005*\"would\" + 0.004*\"think\" + 0.004*\"power\" + 0.004*\"one\" + 0.004*\"com\" + 0.004*\"amp\"'), (19, u'0.009*\"people\" + 0.007*\"miami\" + 0.007*\"com\" + 0.006*\"get\" + 0.006*\"power\" + 0.005*\"would\" + 0.004*\"good\" + 0.004*\"like\" + 0.004*\"https\" + 0.004*\"amp\"'), (16, u'0.009*\"miami\" + 0.007*\"people\" + 0.006*\"like\" + 0.005*\"get\" + 0.005*\"com\" + 0.005*\"one\" + 0.004*\"go\" + 0.004*\"would\" + 0.004*\"deleted\" + 0.004*\"good\"'), (0, u'0.007*\"miami\" + 0.006*\"people\" + 0.006*\"get\" + 0.006*\"one\" + 0.005*\"power\" + 0.005*\"https\" + 0.005*\"would\" + 0.005*\"deleted\" + 0.005*\"com\" + 0.005*\"like\"'), (12, u'0.010*\"miami\" + 0.008*\"people\" + 0.006*\"like\" + 0.006*\"would\" + 0.006*\"deleted\" + 0.005*\"get\" + 0.005*\"power\" + 0.005*\"good\" + 0.005*\"one\" + 0.004*\"https\"'), (5, u'0.008*\"miami\" + 0.008*\"get\" + 0.007*\"people\" + 0.006*\"one\" + 0.005*\"like\" + 0.005*\"power\" + 0.005*\"com\" + 0.005*\"know\" + 0.005*\"deleted\" + 0.004*\"good\"'), (18, u'0.008*\"get\" + 0.007*\"people\" + 0.007*\"miami\" + 0.005*\"like\" + 0.005*\"power\" + 0.005*\"one\" + 0.005*\"would\" + 0.005*\"deleted\" + 0.004*\"com\" + 0.004*\"good\"')]\n"
     ]
    }
   ],
   "source": [
    "ldamodel_25 = gensim.models.ldamodel.LdaModel(corpus, num_topics=25, id2word= dictionary)\n",
    "print(ldamodel_25.print_topics(num_topics=10, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.009*miami + 0.008*people + 0.007*get + 0.007*like + 0.005*com + 0.005*power + 0.005*one + 0.005*would + 0.004*deleted + 0.004*good'), (1, u'0.000*ghost + 0.000*duration + 0.000*1650 + 0.000*unless + 0.000*qualified + 0.000*rest + 0.000*valuable + 0.000*formula + 0.000*lbs + 0.000*albums'), (2, u'0.000*flamingo + 0.000*rituals + 0.000*ripped + 0.000*wynn + 0.000*shoulders + 0.000*playwright + 0.000*hollywood + 0.000*11e6a95042844e2f9ad5a111c5ffd214 + 0.000*plancha + 0.000*coach'), (3, u'0.000*alreayd + 0.000*randomly + 0.000*it_could_happen_here + 0.000*contrary + 0.000*buckling + 0.000*x5q0eoxa_zi + 0.000*effing + 0.000*awoken + 0.000*dispatcher + 0.000*twelve'), (4, u'0.000*americana + 0.000*shouted + 0.000*hurricaneshutter5 + 0.000*goose + 0.000*vlad + 0.000*stimulus + 0.000*device + 0.000*tears + 0.000*mangos + 0.000*20report'), (5, u'0.000*dedicated + 0.000*dilapidated + 0.000*sb + 0.000*citizenship + 0.000*toilet + 0.000*autistic + 0.000*adulting + 0.000*thornton + 0.000*dismisses + 0.000*ape'), (6, u'0.000*adjacent + 0.000*skippy + 0.000*insurance + 0.000*cat4 + 0.000*carajo + 0.000*links + 0.000*ad + 0.000*throwaway + 0.000*84 + 0.000*assume'), (7, u'0.000*revived + 0.000*familiarize + 0.000*kind + 0.000*73rd + 0.000*180 + 0.000*thoo + 0.000*needy + 0.000*wayside + 0.000*cook + 0.000*samez'), (8, u'0.000*earned + 0.000*royally + 0.000*tanker + 0.000*corporates + 0.000*tupperware + 0.000*iheartpublix + 0.000*belp + 0.000*shortsidedness + 0.000*money + 0.000*climate_of_miami'), (9, u'0.000*perez + 0.000*a\\xf1o + 0.000*homelesstrust + 0.000*spicy + 0.000*fawn + 0.000*union + 0.000*lawnmower + 0.000*distance + 0.000*33139 + 0.000*compare')]\n"
     ]
    }
   ],
   "source": [
    "hlda = models.HdpModel(corpus,id2word= dictionary )\n",
    "print(hlda.print_topics(num_topics=10, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.009*miami + 0.008*people + 0.007*get + 0.007*like + 0.005*com + 0.005*power + 0.005*one + 0.005*would + 0.004*deleted + 0.004*good'), (1, u'0.000*says + 0.000*shades + 0.000*tooooo + 0.000*hurdle + 0.000*240 + 0.000*jean + 0.000*tvs + 0.000*aatzpil + 0.000*358 + 0.000*disorders'), (2, u'0.003*hi + 0.000*cream + 0.000*detour + 0.000*paraiso + 0.000*chef + 0.000*oftentimes + 0.000*dump + 0.000*camping + 0.000*homophobic + 0.000*advises'), (3, u'0.000*inelastic + 0.000*exxon + 0.000*bye + 0.000*wrenching + 0.000*jalousie + 0.000*melts + 0.000*go + 0.000*11e6a95042844e2f9ad5a111c5ffd214 + 0.000*louder + 0.000*issuing'), (4, u'0.000*shoulders + 0.000*laying + 0.000*vida + 0.000*crowlers + 0.000*sewn + 0.000*insanity + 0.000*ripping + 0.000*windshield + 0.000*diminishing + 0.000*holds'), (5, u'0.000*divisive + 0.000*cramps + 0.000*emily + 0.000*6z0w20 + 0.000*gorgeous + 0.000*plead + 0.000*breather + 0.000*frost + 0.000*webm + 0.000*courteously'), (6, u'0.000*alabam + 0.000*usps + 0.000*mfc_pref + 0.000*reimbursing + 0.000*fuming + 0.000*vacancies + 0.000*lobs + 0.000*nextradio + 0.000*900sqft + 0.000*lethal'), (7, u'0.000*preexisting + 0.000*iphone + 0.000*gasbuddy + 0.000*zte + 0.000*lech\\xf3n + 0.000*dm1jf4r + 0.000*farc + 0.000*fighters + 0.000*sunpass + 0.000*by3hz8jlflq'), (8, u'0.000*3m8 + 0.000*file + 0.000*discusses + 0.000*satisfaction + 0.000*even + 0.000*stink + 0.000*interconnected + 0.000*hall + 0.000*longshot + 0.000*recyclable'), (9, u'0.000*fh0kf4 + 0.000*investments + 0.000*everyday + 0.000*milankovich + 0.000*raised + 0.000*sustain + 0.000*clintons + 0.000*fedex + 0.000*arbetter + 0.000*version_html')]\n"
     ]
    }
   ],
   "source": [
    "hdp = models.HdpModel(corpus, id2word=dictionary)\n",
    "print(hdp.print_topics(num_topics=10, num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Hurricane Harvey (Houston), Around August 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64354, 26)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#houston_subset_df = houston_df[houston_df['created_at_local'].dt.date > datetime.date(2017, 7, 15)]\n",
    "df_size = houston_df.shape[0]\n",
    "chosen_idx = np.random.choice(df_size, replace=False, size=df_size/4)\n",
    "houston_subset_df = houston_df.iloc[chosen_idx]\n",
    "houston_subset_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 52958\n",
      "Most common unigrams:\n",
      "\".\": 102008\n",
      "\"the\": 63488\n",
      "\",\": 50601\n",
      "\"to\": 44546\n",
      "\"i\": 40524\n",
      "\"a\": 37558\n",
      "\"and\": 34987\n",
      "\"it\": 27325\n",
      "\"you\": 27041\n",
      "\"of\": 26645\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#tokenize words\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "#do some preprocessing before tokenizing\n",
    "#clean_data=get_raw_text(houston_subset_df).lower()\n",
    "\n",
    "#create tokens\n",
    "tokens = nltk.word_tokenize(get_raw_text(houston_subset_df))\n",
    "\n",
    "token_feed = (utils.canonicalize_word(w) for w in tokens)\n",
    "\n",
    "# Collect counts of tokens and assign wordids.\n",
    "vocab = vocabulary.Vocabulary(token_feed)\n",
    "print \"Vocabulary size: %d\" % vocab.size\n",
    "\n",
    "# Print out some (debugging) statistics to make sure everything went\n",
    "# as we expected.  (Unsurprisingly, you should see \"the\" as the most popular word.)\n",
    "print \"Most common unigrams:\"\n",
    "for word, count in vocab.unigram_counts.most_common(10):\n",
    "    print \"\\\"%s\\\": %d\" % (word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'yes', u'maybe', u'phrasing', u\"n't\", u'right']\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing\n",
    "\n",
    "#remove single letters\n",
    "\n",
    "tokens =[i for i in tokens if len(i)>1]\n",
    "\n",
    "\n",
    "tokens = [ i.lower() for i in tokens]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#remove stop words\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# remove stop words from tokens from python stop_tokens\n",
    "stopped_tokens = [i for i in tokens if i not in en_stop]\n",
    "\n",
    "#remove stop words from nltk stop_tokens\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "stopped_tokens = [i for i in tokens if i not in stop_words]\n",
    "\n",
    "\n",
    "#word for word in word_list if word not in stopwords.words('english')]\n",
    "\n",
    "print stopped_tokens[:5]\n",
    "\n",
    "texts= stopped_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 55099\n",
      "Most common unigrams:\n",
      "\"'s\": 16441\n",
      "\"n't\": 15241\n",
      "\"people\": 7203\n",
      "\"like\": 6631\n",
      "\"get\": 6381\n",
      "\"'m\": 5548\n",
      "\"would\": 5047\n",
      "\"'re\": 4561\n",
      "\"one\": 4440\n",
      "\"''\": 4436\n",
      "\"houston\": 4282\n",
      "\"deleted\": 4139\n",
      "\"``\": 4096\n",
      "\"...\": 4044\n",
      "\"https\": 3758\n",
      "\"know\": 3698\n",
      "\"good\": 3526\n",
      "\"go\": 3373\n",
      "\"water\": 3227\n",
      "\"going\": 3101\n"
     ]
    }
   ],
   "source": [
    "# Collect counts of tokens and assign wordids.\n",
    "vocab = vocabulary.Vocabulary(stopped_tokens)\n",
    "print \"Vocabulary size: %d\" % vocab.size\n",
    "\n",
    "# Print out some (debugging) statistics to make sure everything went\n",
    "# as we expected.  (Unsurprisingly, you should see \"the\" as the most popular word.)\n",
    "print \"Most common unigrams:\"\n",
    "for word, count in vocab.unigram_counts.most_common(20):\n",
    "    print \"\\\"%s\\\": %d\" % (word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'yes' u'maybe' u'phrasing' u\"n't\" u'right']\n"
     ]
    }
   ],
   "source": [
    "#convert to an array\n",
    "texts_array = np.asarray(texts)\n",
    "\n",
    "print texts_array[:5]\n",
    "\n",
    "texts_harvey=texts\n",
    "\n",
    "texts_2= [u'hi']\n",
    "\n",
    "texts_all=[texts_harvey, texts_2]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts_all)\n",
    "\n",
    "\n",
    "#convert to corupus\n",
    "corpus = [dictionary.doc2bow(text) for text in texts_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(39, u'0.022*\"\\'s\" + 0.015*\"n\\'t\" + 0.008*\"people\" + 0.008*\"like\" + 0.007*\"would\" + 0.007*\"\\'m\" + 0.006*\"get\" + 0.005*\"...\" + 0.004*\"\\'\\'\" + 0.004*\"``\"'), (18, u'0.021*\"\\'s\" + 0.017*\"n\\'t\" + 0.008*\"like\" + 0.006*\"\\'m\" + 0.006*\"people\" + 0.006*\"houston\" + 0.006*\"get\" + 0.006*\"deleted\" + 0.005*\"would\" + 0.005*\"...\"'), (1, u'0.015*\"n\\'t\" + 0.015*\"\\'s\" + 0.007*\"like\" + 0.007*\"people\" + 0.006*\"get\" + 0.005*\"deleted\" + 0.005*\"\\'m\" + 0.005*\"would\" + 0.004*\"``\" + 0.004*\"\\'\\'\"'), (0, u'0.015*\"\\'s\" + 0.014*\"n\\'t\" + 0.007*\"like\" + 0.007*\"people\" + 0.006*\"get\" + 0.005*\"\\'\\'\" + 0.005*\"would\" + 0.005*\"one\" + 0.005*\"``\" + 0.005*\"...\"'), (20, u'0.018*\"\\'s\" + 0.017*\"n\\'t\" + 0.008*\"people\" + 0.008*\"get\" + 0.007*\"like\" + 0.005*\"\\'re\" + 0.005*\"think\" + 0.005*\"\\'\\'\" + 0.005*\"https\" + 0.005*\"deleted\"'), (13, u'0.018*\"n\\'t\" + 0.011*\"\\'s\" + 0.007*\"people\" + 0.006*\"like\" + 0.005*\"...\" + 0.005*\"\\'m\" + 0.005*\"\\'\\'\" + 0.005*\"would\" + 0.005*\"know\" + 0.005*\"get\"'), (33, u'0.015*\"\\'s\" + 0.013*\"n\\'t\" + 0.010*\"people\" + 0.008*\"get\" + 0.007*\"like\" + 0.007*\"would\" + 0.006*\"houston\" + 0.006*\"one\" + 0.005*\"deleted\" + 0.005*\"\\'re\"'), (41, u'0.014*\"\\'s\" + 0.011*\"n\\'t\" + 0.010*\"get\" + 0.007*\"people\" + 0.006*\"\\'m\" + 0.005*\"like\" + 0.005*\"\\'\\'\" + 0.005*\"``\" + 0.005*\"know\" + 0.004*\"\\'re\"'), (46, u'0.016*\"n\\'t\" + 0.015*\"\\'s\" + 0.008*\"get\" + 0.007*\"\\'m\" + 0.005*\"people\" + 0.005*\"know\" + 0.005*\"one\" + 0.005*\"...\" + 0.005*\"like\" + 0.005*\"deleted\"'), (37, u'0.019*\"\\'s\" + 0.014*\"n\\'t\" + 0.007*\"like\" + 0.007*\"people\" + 0.006*\"get\" + 0.006*\"\\'m\" + 0.005*\"\\'\\'\" + 0.005*\"...\" + 0.005*\"\\'re\" + 0.005*\"houston\"')]\n"
     ]
    }
   ],
   "source": [
    "ldamodel_50 = gensim.models.ldamodel.LdaModel(corpus, num_topics=50, id2word= dictionary)\n",
    "print(ldamodel_50.print_topics(num_topics=10, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u\"0.018*'s + 0.017*n't + 0.008*people + 0.007*like + 0.007*get + 0.006*'m + 0.006*would + 0.005*'re + 0.005*one + 0.005*''\"), (1, u'0.002*hi + 0.000*relaxing + 0.000*calder + 0.000*105k + 0.000*5000 + 0.000*out-right + 0.000*straps + 0.000*unusually + 0.000*fairways + 0.000*//youtu.be/ec5mrgjjan8'), (2, u'0.000*sublease + 0.000*diddly-poo + 0.000*//www.reddit.com/r/houston/comments/6yngn3/houstons_undocumented_immigrants_left_destitute/dmp61n5/ + 0.000*oh.. + 0.000*heather + 0.000*1,000s + 0.000*urging + 0.000***hank + 0.000*licencia + 0.000*equis'), (3, u'0.000*mens + 0.000*mavic + 0.000*00:00:00 + 0.000*eu + 0.000*.5 + 0.000*windmill + 0.000*pro-tip + 0.000*impersonating + 0.000*rpx + 0.000*overflowing'), (4, u\"0.000*2230 + 0.000*releases* + 0.000*confidence + 0.000*srew + 0.000*unfiltered + 0.000*afterlife + 0.000*wsj + 0.000*ditch/i'm-not-sure-technically-what-to-call-it + 0.000*//imgur.com/z6azbhl + 0.000*//goo.gl/maps/4ywt92lp3fk\"), (5, u'0.000*paranoid1123 + 0.000*281-948-2714 + 0.000*grabber + 0.000*swelled + 0.000*//giphy.com/gifs/kate-upton-terry-richardson-cat-daddy-hscrnsmg8gdfi + 0.000*ceasing + 0.000*mouthful + 0.000*abridged + 0.000*dg + 0.000*retweeted'), (6, u\"0.000*algebra + 0.000*rubbernecking + 0.000*non-samsung + 0.000*crisis + 0.000*'called + 0.000*dispatchers + 0.000*1400sq + 0.000*14.5 + 0.000*1915 + 0.000*spaghetti\"), (7, u'0.000*negate + 0.000**rabble + 0.000*carroll + 0.000*yeap + 0.000*kimchi + 0.000*ada + 0.000*endline + 0.000*buy + 0.000*richy-rich + 0.000**biblical*'), (8, u'0.000*pumps + 0.000*property/yard/porch + 0.000*notary + 0.000*16oz + 0.000*//www.reddit.com/r/jokes/comments/3z0nz5/an_irishman_walks_into_a_job_interview/ + 0.000*/r/stopdrinking + 0.000*repugnant + 0.000*/r/denver + 0.000*intruders + 0.000*dude-who-is-trying-to-make-mad-men-an-adjective'), (9, u'0.000*//www.reddit.com/r/houston/comments/6vj3c5/gook_movie + 0.000*diversified + 0.000*authorized + 0.000*p.m. + 0.000*met/married + 0.000*spits + 0.000*jpegs + 0.000*barbecue + 0.000*//global100.adl.org/ + 0.000*archipelago*')]\n"
     ]
    }
   ],
   "source": [
    "hlda = models.HdpModel(corpus,id2word= dictionary )\n",
    "print(hlda.print_topics(num_topics=10, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u\"0.018*'s + 0.017*n't + 0.008*people + 0.007*like + 0.007*get + 0.006*'m + 0.006*would + 0.005*'re + 0.005*one + 0.005*''\"), (1, u\"0.000*probing + 0.000*showtime + 0.000*f**d + 0.000*o'rourke + 0.000*invision + 0.000*\\U0001f602\\U0001f602\\U0001f602 + 0.000*probation + 0.000*detained + 0.000*//www.nhc.noaa.gov + 0.000*stuffing\"), (2, u'0.000*showbiz + 0.000*steroids + 0.000*ll=29.896615075309754 + 0.000*copper + 0.000*poll + 0.000*med + 0.000*messaged + 0.000*euless + 0.000*www.connollyshireman.com + 0.000*pappasito'), (3, u'0.000*salons + 0.000*pro-net + 0.000*brah + 0.000*top** + 0.000*hitchhiking + 0.000*2097/07184 + 0.000*to.. + 0.000*8/25 + 0.000*dingaling + 0.000*to=remindmebotwrangler'), (4, u'0.000*somalian + 0.000*ejs + 0.000*understandings + 0.000*//en.wikipedia.org/wiki/levee_breach + 0.000*jac + 0.000*jesse + 0.000*afaik + 0.000*abdul + 0.000*tcr + 0.000*sadistic'), (5, u'0.000*13,300 + 0.000*frequenter + 0.000*acoustic + 0.000*edgy + 0.000*thoughtful + 0.000*info\\u2026 + 0.000*cletus + 0.000*dgn + 0.000*murky + 0.000*lansing'), (6, u'0.000*u/osnapmillertime + 0.000*//artcarmuseum.com/2017/08/13/trump-this/ + 0.000*mesquite + 0.000*fr\\xedo + 0.000*~~in + 0.000*humid + 0.000*8/29/2017 + 0.000*.04 + 0.000*fury + 0.000*qid=1496868214'), (7, u'0.000*528 + 0.000*//i.imgur.com/0uxsveq.jpg + 0.000*5424 + 0.000*rung + 0.000*donation + 0.000*harm-reduction + 0.000*overlarge + 0.000*wl5=9025393 + 0.000*today + 0.000*westview/britt'), (8, u'0.000*capacitors + 0.000*sadly + 0.000*contaminated + 0.000*drive/run/walk + 0.000*starts + 0.000*cheap-as-fuck + 0.000*dumb + 0.000*77018-9998 + 0.000*flickers + 0.000*horrors'), (9, u'0.000*commenter + 0.000*condense + 0.000*keymod + 0.000*//www.reddit.com/user/that_indian_girl/comments/ + 0.000*cat* + 0.000*neanderthal + 0.000*ex-californians + 0.000*diatomaceous + 0.000*wilderness + 0.000*kung-fu')]\n"
     ]
    }
   ],
   "source": [
    "hlda = models.HdpModel(corpus,id2word= dictionary )\n",
    "print(hlda.print_topics(num_topics=10, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Las Vegas Shooting, October 1, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#References\n",
    "https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/topic_coherence_model_selection.ipynb\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
