{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import make_moons\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 20\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Sanitizes the text by removing front and end punctuation, \n",
    "#making words lower case, and removing any empty strings.\n",
    "def get_text_sanitized(tweet):\n",
    "    return ' '.join([w.lower().strip().rstrip(string.punctuation)\\\n",
    "        .lstrip(string.punctuation).strip()\\\n",
    "        for w in tweet.replace('\\xe2\\x80\\xa6', '').split(\" \")\\\n",
    "        if w.strip().rstrip(string.punctuation).strip()])\n",
    "\n",
    "#Gets the text, clean it, make it lower case, stem the words, and split\n",
    "#into a vector. Also, remove stop words.\n",
    "def get_text_normalized(tweet):\n",
    "    #Sanitize the text first.\n",
    "    text = get_text_sanitized(tweet).split()\n",
    "    \n",
    "    #Remove the stop words.\n",
    "    text = [t for t in text if t not in [stopwords.words('english')] ]\n",
    "\n",
    "    return text\n",
    "    \n",
    "    #Stemmer gets upset at a lot of tweets\n",
    "    #Create the stemmer.\n",
    "    stemmer = LancasterStemmer()\n",
    "    \n",
    "    #Stem the words.\n",
    "    return [stemmer.stem(t) for t in text]\n",
    "\n",
    "def purge_urls(tweet):\n",
    "    return re.sub('http[s]?://(www. )?(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', tweet)\n",
    "\n",
    "def sanitize_dataset(dataset):\n",
    "    sentences = []\n",
    "    for sentence in dataset:\n",
    "        sentences.append(get_text_normalized(purge_urls(sentence)))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### vegas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### vegas before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected string or buffer\n",
      "expected string or buffer\n"
     ]
    }
   ],
   "source": [
    "pattern = \"http\"\n",
    "dirs = glob.glob(\"data/data_Las_Vegas/2017-09*.csv\")\n",
    "sentences = []\n",
    "for dir_ in dirs:\n",
    "    try:\n",
    "        df = pd.read_csv(dir_, delimiter=\";\")\n",
    "\n",
    "        new_sentences = list(df['text'].values)\n",
    "        for sentence in new_sentences:\n",
    "            sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "            sentence = re.sub(\" \\d+\", '', sentence)\n",
    "            sentence = re.sub(r'\\w*\\d\\w*', '', sentence)\n",
    "            sentence = re.sub('http[s]?://(www. )?(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', sentence)\n",
    "            sentences.append(sentence)\n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: vegas las strip north bellagio venetian cosmopolitan downtown wynn night baby center rock convention weekend welcome fabulous em paris club\n",
      "Topic #1: bit ly vegastraffic xxmewb accident http blvd rd clark nb ave sb reported approaching ramp right beltway eb dr sahara\n",
      "Topic #2: beautiful life lifeisbeautiful festival weekend amazing lib friends lifeisbeautifulfestival art people gorillaz best live experience truly like fun really ready\n",
      "Topic #3: com twitter pic http nfl raidernation mp raiders tour home circlepix week https oakland dlvr listing washington retweet listed jets\n",
      "Topic #4: lasvegas lasvegasstrip lifeisbeautiful like usa bellagio hiring vegas strip home vivalasvegas travel jobs flamingo bar world morning place ready great\n",
      "Topic #5: casino hotel resort paris mirage aria flamingo hollywood planet orleans luxor excalibur bay mandalay york rock spa hard island bellagio\n",
      "Topic #6: just posted photo video like stratosphere got planet fitness bar resort cause live bay mandalay club home park won hollywood\n",
      "Topic #7: time great fun thank got night amazing having good lifeisbeautiful weekend year took favorite ago ve friends saw work lunch\n",
      "Topic #8: nevada north las henderson valley spring enterprise paradise good sunrise way summerlin southern lol em south look manor lets trails\n",
      "Topic #9: beer untp drinking http photo ipa tenayacreek bangerbrewing ale beerhaus light hop little parisvegas double area brewing house golden company\n",
      "Topic #10: day great office favorite pool week lifeisbeautiful good start like coffee national everyday days sunday swim work lets having blessed\n",
      "Topic #11: nv henderson las lunch tmobilearena bar mccarran lasairport international hi ranch septh forecast sunny buffet lo village airport paradise station\n",
      "Topic #12: love thank don place amazing need family people thanks year bestoftheday follow little school really wish cirque anniversary like fun\n",
      "Topic #13: tonight party nightclub inside lets come git night live catch going septh forecast sunny xslasvegas hi weekend lo hakkasanlv takes\n",
      "Topic #14: new york check placed listing got sign shirt brand hair hotel old stuff world home look style realty city went\n",
      "Topic #15: en la que el una para del los mi venetian esta por mejor las lo te se acaba mas noche\n",
      "Topic #16: today got work good ve like years open hi great septh sunny forecast say lo ago home pm working special\n",
      "Topic #17: repost get_repost night jlo live olympia weekend yes september month attention thank tomorrow ricky_martin happening episode iheartfestival tbt friends thanks\n",
      "Topic #18: grand mgm mgmgrand pool caneloggg garden arena las desert downtown boxing wedding family thank concert poolside style vegas canyon tbt\n",
      "Topic #19: happy birthday best shout wishing friday talented monday happyst coffee celebrating friends friend brother excited thank hope perfect resort kids\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### vegas after \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 20\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error tokenizing data. C error: Expected 10 fields in line 296, saw 11\n",
      "\n",
      "expected string or buffer\n",
      "Error tokenizing data. C error: Expected 10 fields in line 239, saw 11\n",
      "\n",
      "expected string or buffer\n",
      "Error tokenizing data. C error: Expected 10 fields in line 1755, saw 11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pattern = \"http\"\n",
    "dirs = glob.glob(\"data/data_Las_Vegas/2017-10*.csv\")\n",
    "sentences = []\n",
    "for dir_ in dirs:\n",
    "    try:\n",
    "        df = pd.read_csv(dir_, delimiter=\";\")\n",
    "#         df = df[~df.text.str.contains(pattern)]\n",
    "        new_sentences = list(df['text'].values)\n",
    "        for sentence in new_sentences:\n",
    "#             regex = re.compile('[^a-zA-Z]')\n",
    "#             sentence = regex.sub(sentence, regex)\n",
    "            sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "            sentence = re.sub(\" \\d+\", '', sentence)\n",
    "            sentence = re.sub(r'\\w*\\d\\w*', '', sentence)\n",
    "\n",
    "            sentences.append(sentence)\n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: vegas las strip bellagio venetian cosmopolitan north wynn en downtown grand good palazzo mgm vegasbaby fabulous sign welcome view weekend\n",
      "Topic #1: bit ly vegastraffic xxmewb accident http blvd rd clark ave nb sb reported approaching right eb dr charleston wb west\n",
      "Topic #2: swarmapp nv las henderson vegas bar center lasairport cafe pic twitter grill lunch home restaurant station burger starbucks buffet picking\n",
      "Topic #3: beer untp drinking http photo ipa khourysfinewine ale hopnutsbrewing zombies bottle share tenayacreek lager stout light bangerbrewing hop bar eagle\n",
      "Topic #4: lasvegas bellagio lasvegasstrip sincity en usa lv tour sema home hiring travel depechemode fremont mandalaybay vegas wynn fun circlepix dtlv\n",
      "Topic #5: casino hotel resort rock luxor hard mirage aria mandalay bay spa paris planet hollywood red york excalibur rio island suite\n",
      "Topic #6: just posted photo video got park like want listed palace nightclub caesars retweet center red restaurant henderson world store don\n",
      "Topic #7: nevada north las paradise henderson vegas spring valley enterprise summerlin en south que morning dog home work got like sunrise\n",
      "Topic #8: tonight party nightclub come lets inside hakkasanlv lo hi sunny git forecast today ready join octth clear marqueelv vip live\n",
      "Topic #9: twitter pic nfl mp raidernation http raiders tour home circlepix week oakland ravens vs retweet listed dlvr listing virtual newest\n",
      "Topic #10: vegasstrong welcome prayforvegas fabulous mandalay bay sign city mandalaybay vegasgoldenknights proud resort lasvegasstrip prayforlasvegas strong goldenknights game memorial vegas church\n",
      "Topic #11: love city life thank bestoftheday people instacool instago support birthday hate friends place beatles gotta family man amazing follow all_shots\n",
      "Topic #12: day today beautiful good great best little work fun game thank happy sema come tomorrow amazing got started make awesome\n",
      "Topic #13: time great life year good game long spend spending friends vegas make like years selfie having start pool got eat\n",
      "Topic #14: new york check listing placed hotel sign products brand music got today fall old video set trying coming realty thank\n",
      "Topic #15: happy halloween birthday weekend costume town happyhalloween pumpkin safe best october party wishing fun shout come month little celebrate ready\n",
      "Topic #16: arena mobile grand mgm vegasgoldenknights garden genblue knights game depechemode hockey nhl goldenknights depeche mode tmobilearena vgk vegasgoesgold ice golden\n",
      "Topic #17: airport mccarran international lasairport home vegas good bye hello vacation arrived landed president baby heading flight ready sweet finally goodbye\n",
      "Topic #18: night great fun amazing saturday friday thanks tomorrow good thank repost vegas party thursday get_repost date little times sunday dinner\n",
      "Topic #19: bz ufc sam thank boyd fight stadium mightymouseufc hakkasan red nightclub tonyfergusonxt dinner right win north lol fun ready ve\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### houston before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error tokenizing data. C error: Expected 10 fields in line 70, saw 11\n",
      "\n",
      "expected string or buffer\n",
      "expected string or buffer\n"
     ]
    }
   ],
   "source": [
    "pattern = \"http\"\n",
    "dirs = glob.glob(\"data/data_Houston/2017-08-0*.csv\") + glob.glob (\"data/data_Houston/2017-08-10.csv\") + glob.glob(\"data/data_Houston/2017-08-11.csv\")  + glob.glob(\"data/data_Houston/2017-08-12.csv\")  +glob.glob(\"data/data_Houston/2017-08-13.csv\") +glob.glob(\"data/data_Houston/2017-08-14.csv\") +glob.glob(\"data/data_Houston/2017-08-15.csv\") + glob.glob(\"data/data_Houston/2017-08-16.csv\")\n",
    "sentences = []\n",
    "for dir_ in dirs:\n",
    "    try:\n",
    "        df = pd.read_csv(dir_, delimiter=\";\")\n",
    "#         df = df[~df.text.str.contains(pattern)]\n",
    "        new_sentences = list(df['text'].values)\n",
    "        for sentence in new_sentences:\n",
    "#             regex = re.compile('[^a-zA-Z]')\n",
    "#             sentence = regex.sub(sentence, regex)\n",
    "            sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "            sentence = re.sub(\" \\d+\", '', sentence)\n",
    "            sentence = re.sub(r'\\w*\\d\\w*', '', sentence)\n",
    "\n",
    "            sentences.append(sentence)\n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: traffic delay stop mins fwy cleared accident outbound lp inbound hwy sw katy rd ly bit stall northside nb gulf\n",
      "Topic #1: swarmapp tx houston club pasadena intercontinental airport george pearland bush city brunch checkin sam sports starbucks ubercheckin automatically uber grill\n",
      "Topic #2: bubly http beds baths tx st dr pearland ln pasadena bath rd houston porte ct deer la way lake bellaire\n",
      "Topic #3: beer untp drinking http photo ipa flying ale hops hop saintarnold conservatoryhtx premiumdraught summer grill heights meet west casa better\n",
      "Topic #4: twitter pic dlvr rt status http astros shit did way white lol people road circlepix fun george cool local try\n",
      "Topic #5: houston texas southeast westside htown downtown htx north museum night en arts work sunday sundayfunday live mi la good fine\n",
      "Topic #6: just posted photo video club fitness heights trying galleria live know pearland listed free petco events thanks store say game\n",
      "Topic #7: day family wednesday special amazing school awesome make night beautiful monday repost free fun come great pearland get_repost way church\n",
      "Topic #8: new repost shop night shades year coming get_repost team live wednesday music saloon outlaw tuesday johnny link share black vibes\n",
      "Topic #9: tonight chance storm lo pm hi forecast augth sugar land bar tx wednesday sharpstown monday wed sun tuesday thursday saturday\n",
      "Topic #10: right blocks left lane bit ly accident shoulder lanes traffic fwy stop sam http inbound blocked hwy outbound tollway wb\n",
      "Topic #11: time food fun girl available friend good work best light having photo youtube album vs coffee great young excited arts\n",
      "Topic #12: park minute maid astros mlb deer baseball game great blue good view dlvr end want memorial life vs win place\n",
      "Topic #13: today week sunday did great come beautiful awesome book funday face lunch trying came link bio stop pretty shoot tomorrow\n",
      "Topic #14: happy birthday hour friday little thank family life girls celebrating special come big year god anniversary night pretty weekend kitchen\n",
      "Topic #15: don know miss want play work bxy people repost try really just person tonight trying tomorrow gonna life training damn\n",
      "Topic #16: home tour circlepix drive realestate listing http newest virtual tx listed listings pic humble retweet check houston looking run oak\n",
      "Topic #17: got ve night finally job workout book just shit fun run special post lil amazing repost live album guys lunch\n",
      "Topic #18: like work fitness center lol look looking people book thank fuck club good year man doesn big share training pick\n",
      "Topic #19: love fun town team music taking baby ll girls say white power know old enjoy thanks city ve houston number\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### houston after "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected string or buffer\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "expected string or buffer\n"
     ]
    }
   ],
   "source": [
    "pattern = \"http\"\n",
    "dirs = glob.glob(\"data/data_Houston/2017-08-2*.csv\") + glob.glob(\"data/data_Houston/2017-08-3*.csv\")\n",
    "#print dirs\n",
    "sentences = []\n",
    "for dir_ in dirs:\n",
    "    try:\n",
    "        df = pd.read_csv(dir_, delimiter=\";\")\n",
    "        new_sentences = list(df['text'].values)\n",
    "        for sentence in new_sentences:\n",
    "            sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "            sentence = re.sub(\" \\d+\", '', sentence)\n",
    "            sentence = re.sub(r'\\w*\\d\\w*', '', sentence)\n",
    "\n",
    "            sentences.append(sentence)\n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: houston texas westside pray share southeast downtown byb help city htown en day like god byt prayforhouston time eastside flood\n",
      "Topic #1: water bit ly high lanes traffic main affecting http fwy sb hwy wb nb lp downtown baytown eb right harris\n",
      "Topic #2: bubly http beds baths beer tx st untp dr closed drinking astros shelter flooding pasadena pearland new ln porte relief\n",
      "Topic #3: harvey hurricane houston got rain needs relief prayers center look today flooded friday come good george stuck open thing home\n",
      "Topic #4: hurricaneharvey safe prayforhouston stay houston need park downtown htx flooded rain ready houstonstrong dry buffalo going water home getting prayers\n",
      "Topic #5: just posted photo video stadium hard downtown got houston house southeast getting la didn meyerland beer untp old don say\n",
      "Topic #6: twitter pic status rt like god tour home astros looking getting time pray didn great http know going friday people\n",
      "Topic #7: repost get_repost help people prayforhouston thanks world thank htx need let food family strong rescue game know better right share\n",
      "Topic #8: traffic stop mins delay accident fwy cleared outbound sam lp right blocks hwy rd eastside tollway inbound lane sb eb\n",
      "Topic #9: tx swarmapp houston park land beds pasadena sugar bayou baths storm e_no uv usgs gov waterdata nwis sit st tonight\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### puerto rico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/data_San_Juan/2017-09-06.csv', 'data/data_San_Juan/2017-09-07.csv', 'data/data_San_Juan/2017-09-05.csv', 'data/data_San_Juan/2017-09-04.csv', 'data/data_San_Juan/2017-09-01.csv', 'data/data_San_Juan/2017-09-03.csv', 'data/data_San_Juan/2017-09-02.csv', 'data/data_San_Juan/2017-09-09.csv', 'data/data_San_Juan/2017-09-08.csv']\n"
     ]
    }
   ],
   "source": [
    "pattern = \"http\"\n",
    "dirs = glob.glob(\"data/data_San_Juan/2017-09-0*.csv\")\n",
    "sentences = []\n",
    "for dir_ in dirs:\n",
    "    try:\n",
    "        df = pd.read_csv(dir_, delimiter=\";\")\n",
    "        new_sentences = list(df['text'].values)\n",
    "        for sentence in new_sentences:\n",
    "            sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "            sentence = re.sub(\" \\d+\", '', sentence)\n",
    "            sentence = re.sub(r'\\w*\\d\\w*', '', sentence)\n",
    "            sentences.append(sentence)\n",
    "    except Exception as e:\n",
    "        print e\n",
    "print dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: juan san argentina marquesado capital todo que hoy foto te es gracias feliz estadio dia el la del club boca\n",
      "Topic #1: la hoy san es sabado siempre del juan gracias foto feliz estadio yo el lo dia club capital boca bicentenario\n",
      "Topic #2: que lo es pero todo siempre yo por hoy quiero mas capital dia el juan del club boca estadio feliz\n",
      "Topic #3: el hoy quiero boca yo foto es que capital feliz la juan gracias estadio lo dia del club bicentenario las\n",
      "Topic #4: del bicentenario siempre estadio juan sabado san que club hoy gracias foto feliz yo es el las dia capital boca\n",
      "Topic #5: te siempre quiero estadio sabado pero hoy que gracias juan dia la del el es club capital feliz foto boca\n",
      "Topic #6: por gracias lo quiero siempre sabado la todo del estadio juan hoy bicentenario foto feliz boca es el dia club\n",
      "Topic #7: sanjuan estadio boca bicentenario san yo juan gracias la hoy foto feliz el es lo dia del club capital las\n",
      "Topic #8: una foto sabado bicentenario marquesado lo feliz las la juan hoy gracias boca capital es el dia del club estadio\n",
      "Topic #9: mi siempre yo marquesado boca todo argentina es hoy bicentenario capital club del dia el las estadio feliz foto gracias\n",
      "Topic #10: las pero sabado hoy san juan yo es bicentenario estadio lo el capital feliz foto dia gracias del club la\n",
      "Topic #11: mas boca que juan una san capital hoy yo gracias foto feliz es estadio las el dia del club bicentenario\n",
      "Topic #12: twitter pic hoy lo juan yo estadio la gracias foto feliz es el dia del club capital boca bicentenario las\n",
      "Topic #13: para capital boca siempre hoy pero gracias juan san sabado todo estadio foto feliz yo es el la del club\n",
      "Topic #14: feliz todo lo siempre quiero estadio yo la juan hoy gracias foto el es dia del club capital boca bicentenario\n",
      "Topic #15: los que gracias estadio la las juan hoy foto feliz yo es el dia del club capital boca bicentenario lo\n",
      "Topic #16: club quiero yo las la juan hoy gracias foto feliz estadio los es el dia del capital boca bicentenario lo\n",
      "Topic #17: dia lo del todo argentina capital club boca bicentenario el los estadio feliz foto gracias hoy juan la las es\n",
      "Topic #18: si yo marquesado todo por pero juan hoy gracias foto feliz estadio es las el dia del club capital boca\n",
      "Topic #19: se siempre quiero yo es la juan hoy gracias foto feliz estadio el lo dia del club capital boca bicentenario\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### maria  during"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = \"http\"\n",
    "dirs = glob.glob(\"data/data_San_Juan/2017-09-2*.csv\")\n",
    "sentences = []\n",
    "for dir_ in dirs:\n",
    "    try:\n",
    "        df = pd.read_csv(dir_, delimiter=\";\")\n",
    "        new_sentences = list(df['text'].values)\n",
    "        for sentence in new_sentences:\n",
    "            sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "            sentence = re.sub(\" \\d+\", '', sentence)\n",
    "            sentence = re.sub(r'\\w*\\d\\w*', '', sentence)\n",
    "\n",
    "            sentences.append(sentence)\n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: juan san argentina lucianopereyra marquesado dia vida foto lo domingo quiero club como del la el es feliz capital al\n",
      "Topic #1: que lo mute la para el juan foto feliz es vida domingo del como club capital argentina al dia los\n",
      "Topic #2: la lucianopereyra que una mute vida san es foto feliz el dia domingo del como club capital argentina al juan\n",
      "Topic #3: el domingo para dia la juan foto feliz es vida lo del como club capital argentina al las los una\n",
      "Topic #4: mi vida lucianopereyra para marquesado san es feliz foto el domingo la dia del como club capital argentina al juan\n",
      "Topic #5: te quiero mas vida como que feliz mute san argentina capital club juan del dia al domingo el es foto\n",
      "Topic #6: los quiero san mute es domingo juan foto feliz el del dia las como club capital argentina al la vida\n",
      "Topic #7: sabado mute para vida domingo juan foto feliz es el dia las del como club capital argentina al la los\n",
      "Topic #8: por vida quiero una lo club capital como del la argentina domingo el es feliz foto juan al dia los\n",
      "Topic #9: se vida como dia mute lucianopereyra te foto feliz es el del domingo la club capital argentina al juan los\n",
      "Topic #10: capital juan san domingo como mute dia para te foto feliz es el vida la club argentina al del los\n",
      "Topic #11: feliz dia quiero para domingo vida san argentina juan foto es el del las como club capital al la los\n",
      "Topic #12: una foto acaba publicar marquesado mute vida que quiero al argentina capital club como del dia domingo el es feliz\n",
      "Topic #13: las quiero vida como mute foto el juan feliz es dia domingo del club capital argentina al la los lo\n",
      "Topic #14: mas para una lucianopereyra argentina lo vida foto feliz es el la domingo dia del como club capital al juan\n",
      "Topic #15: del dia san juan como vida lo una mas club capital argentina la domingo el es feliz foto al los\n",
      "Topic #16: primavera feliz lo la juan foto es el domingo vida dia del como club capital argentina al las los una\n",
      "Topic #17: al como quiero una el la juan foto feliz es vida domingo las del club capital argentina dia los lo\n",
      "Topic #18: es como lo para una mute el juan foto feliz vida la dia del club capital argentina al domingo los\n",
      "Topic #19: club vida domingo la juan foto feliz es el dia lo del como capital argentina al las los una publicar\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Miami "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before Irma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error tokenizing data. C error: Expected 10 fields in line 987, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 974, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 846, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 579, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 644, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 425, saw 12\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 1105, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 276, saw 11\n",
      "\n",
      "expected string or buffer\n"
     ]
    }
   ],
   "source": [
    "pattern = \"http\"\n",
    "dirs = glob.glob(\"data/data_Miami/2017-08*.csv\")\n",
    "sentences = []\n",
    "for dir_ in dirs:\n",
    "    try:\n",
    "        df = pd.read_csv(dir_, delimiter=\";\")\n",
    "        new_sentences = list(df['text'].values)\n",
    "        for sentence in new_sentences:\n",
    "            sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "            sentence = re.sub(\" \\d+\", '', sentence)\n",
    "            sentence = re.sub(r'\\w*\\d\\w*', '', sentence)\n",
    "            sentence = re.sub('http[s]?://(www. )?(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', sentence)\n",
    "            sentences.append(sentence)\n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: miami international airport mia miamibeach southbeach downtown brickell city vacation beach home north vibes iflymia tbt like good fontainebleau miamilife\n",
      "Topic #1: chance tonight storm pm hi forecast lo augth fl cloudy partly showers tue wednesday sunny storms mon sat thu tuesday\n",
      "Topic #2: bit ly http sfltraffic blocked lane sb st accident nb disabled vehicle sr left ramp right express tpke cleared nwth\n",
      "Topic #3: en una los mi acaba publicar las foto doral hoy este del ya por park noche tu kendall esta nuestro\n",
      "Topic #4: com twitter pic http tour https home realestate keyes listing net dlvr utm_source utm_medium tecnohoy status circlepix virtual looking shop\n",
      "Topic #5: beach south usa hotel miami southbeach miamibeach sunny ocean isles fontainebleau summer hollywood em hallandale north drive sea life riu\n",
      "Topic #6: repost get_repost saturday amazing party today tonight tbt join ladies app coming meet tomorrow una esta tune augustth itunes class\n",
      "Topic #7: florida usa miami doral key hialeah kendall biscayne em tpke arts lakes ext district west miamibeach beach monday summer man\n",
      "Topic #8: just posted photo video park like restaurant gardens fitness golden doral studio center say fit clevelander think cafe brickell dadeland\n",
      "Topic #9: la por del es vida mejor noche si anoche hoy su dios se fitness mas cuando oportunidad para esta gracias\n",
      "Topic #10: wynwood art district arts walls wynwoodwalls streetart design museum perez neutrogena beer today life artist little miamideep world better vida\n",
      "Topic #11: humidity windmph clouds current mph weather temperature pressuremb rain thunderstorm scattered downf light sky clear broken upf hialeah tamiami miramar\n",
      "Topic #12: day happy today birthday weekend great labor beautiful saturday come best party thank make friday like national good open august\n",
      "Topic #13: el para los del es hoy dia por todo se sabado viernes nuestro una esta mejor ver al gracias desde\n",
      "Topic #14: fl coral gables isles sunny north park restaurant doral beach miramar aventura iflymia miami hialeah pic bar west bay city\n",
      "Topic #15: new link bio music check shop available brand video post single way online today instore featuring excited coming site summer\n",
      "Topic #16: love life birthday live happy need world family best friends really enjoy don music amor come think like doesn listing\n",
      "Topic #17: time great amazing know today long making let life year ocean spend people summer week stadium game pool saturday think\n",
      "Topic #18: que mi es te se lo por los mas una para tu las hoy esta este gracias como al dia\n",
      "Topic #19: traffic expy nwth sr delay mins st palmetto accident stop ave blocked dolphin cleared slow sfltraffic nb lane wb doral\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### during/after irma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected string or buffer\n",
      "expected string or buffer\n",
      "Error tokenizing data. C error: Expected 10 fields in line 336, saw 11\n",
      "\n",
      "expected string or buffer\n",
      "Error tokenizing data. C error: Expected 10 fields in line 357, saw 11\n",
      "\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "Error tokenizing data. C error: Expected 10 fields in line 706, saw 11\n",
      "\n",
      "expected string or buffer\n",
      "expected string or buffer\n"
     ]
    }
   ],
   "source": [
    "pattern = \"http\"\n",
    "dirs = glob.glob(\"data/data_Miami/2017-09*.csv\")\n",
    "sentences = []\n",
    "for dir_ in dirs:\n",
    "    try:\n",
    "        df = pd.read_csv(dir_, delimiter=\";\")\n",
    "        new_sentences = list(df['text'].values)\n",
    "        for sentence in new_sentences:\n",
    "            sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "            sentence = re.sub(\" \\d+\", '', sentence)\n",
    "            sentence = re.sub(r'\\w*\\d\\w*', '', sentence)\n",
    "            sentence = re.sub('http[s]?://(www. )?(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', sentence)\n",
    "            sentences.append(sentence)\n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: miami airport downtown international miamibeach mia brickell city em north tbt southbeach gardens iflymia good dade hurricaneirma night today live\n",
      "Topic #1: sfltraffic bit ly http sr blocked accident expy nb lane st nwth sb rd palmetto disabled cleared vehicle ave right\n",
      "Topic #2: chance storm tonight hi forecast lo pm septh fl showers sat mon storms sunday tue wednesday heights tuesday thu friday\n",
      "Topic #3: en mi una los doral hoy brickell acaba publicar del por te foto este ya esta mexico para las dios\n",
      "Topic #4: com twitter pic http tour home realestate https dlvr net keyes utm_medium utm_source tecnohoy listing circlepix looking virtual buyer status\n",
      "Topic #5: irma hurricane hurricaneirma ready safe post huracan like miamibeach got stay relief week storm aftermath brickell southflorida thank help power\n",
      "Topic #6: beach south sunny miamibeach southbeach isles miami hallandale north ocean usa isle tbt em hollywood pointe drive hotel fontainebleau vacation\n",
      "Topic #7: repost get_repost dracobanks columbinerecords tonight live fittrunoficial night party tomorrow sunday friday thursday video come join thanks ready link help\n",
      "Topic #8: florida doral kendall miami hurricaneirma usa north biscayne key tpke beach aventura hialeah west coral miramar ext lakes gables arts\n",
      "Topic #9: just posted photo video brickell club park doral way church hollywood got like beer port university health island cutler cruise\n",
      "Topic #10: humidity windmph clouds weather current pressuremb temperature mph rain downf scattered broken thunderstorm light upf sky clear moderate overcast haze\n",
      "Topic #11: la es vida del por despues mejor gracias para te ya como tormenta hoy se tu asi calma semana esta\n",
      "Topic #12: day happy labor today birthday beautiful weekend great monday laborday let thursday best party good amazing open hope week fun\n",
      "Topic #13: fl doral aventura gables coral isles park hallandale sunny iflymia city bar center north hialeah miramar kendall mall starbucks market\n",
      "Topic #14: que es los para lo se mi mas las por te esta como una gracias todos este nos tu hoy\n",
      "Topic #15: time good today year great family fun work wizard party getting know weekend trying got join real act pool dinner\n",
      "Topic #16: wynwood art district walls arts wynwoodwalls design streetart em life perez museum work miami arte house truth thanks picture don\n",
      "Topic #17: love thank life city birthday best home amazing things happy selfie night listing gotta beautiful place like good people hope\n",
      "Topic #18: el por del las huracan es esta mundo para todo los hoy se como dia mas al despues paso huracanirma\n",
      "Topic #19: new music check link bio video fall year world coming shop youtube online brand alert single soon arrivals artists project\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sandy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Before sandy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error tokenizing data. C error: Expected 10 fields in line 687, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 41, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 683, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 553, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 240, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 473, saw 11\n",
      "\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "Error tokenizing data. C error: Expected 10 fields in line 34, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 37, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 242, saw 12\n",
      "\n",
      "expected string or buffer\n",
      "Error tokenizing data. C error: Expected 10 fields in line 3, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 856, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 105, saw 11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pattern = \"http\"\n",
    "dirs = glob.glob(\"data/data_New_York_City/2012-10-0*.csv\")+ glob.glob(\"data/data_New_York_City/2012-10-1*.csv\")+glob.glob(\"data/data_New_York_City/2012-10-20*.csv\")+glob.glob(\"data/data_New_York_City/2012-10-21*.csv\")+glob.glob(\"data/data_New_York_City/2012-10-22*.csv\")+ glob.glob(\"data/data_New_York_City/2012-10-23*.csv\")\n",
    "sentences = []\n",
    "for dir_ in dirs:\n",
    "    try:\n",
    "        df = pd.read_csv(dir_, delimiter=\";\")\n",
    "\n",
    "        new_sentences = list(df['text'].values)\n",
    "        for sentence in new_sentences:\n",
    "            sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "            sentence = re.sub(\" \\d+\", '', sentence)\n",
    "            sentence = re.sub(r'\\w*\\d\\w*', '', sentence)\n",
    "            sentence = re.sub('http[s]?://(www. )?(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', sentence)\n",
    "            sentences.append(sentence)\n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: http instagr park street newyork hall ly bar photo dinner bit posted city square art music day hotel house theater\n",
      "Topic #1: new york ny bar city square street hotel hall lincoln times cafe west shop theatre club kitchen st world grill\n",
      "Topic #2: pic twitter com path event http great tonight party right dinner awesome theatre birthday better food oh work el cool\n",
      "Topic #3: like feel people look really nice shit looks make girl try fucking stop away open im niggas lmaoo getting ll\n",
      "Topic #4: just foursquare mayor posted photo ousted think day saw came haha did fuck face wanna oh come noticed baby literally\n",
      "Topic #5: love baby izod thanks guys lmfao friend omg haha fuck true nigga great team ya damn face tho missing forever\n",
      "Topic #6: lol know ass right yea im tho think hard yeah smh stop text ya ll cause did use okay dont\n",
      "Topic #7: brooklyn ny bridge barclays bowl park restaurant pic services studios dog free st club mall family fitness thanks church bronx\n",
      "Topic #8: time hall party fucking day great tonight week best people think place fucked god mom stay nap game webster ok\n",
      "Topic #9: que la el es tu te se lo en como mi yo si por mas pero le una para las\n",
      "Topic #10: don know want shit say fuck lmao care think people need talk really anymore hate dont play right wanna hear\n",
      "Topic #11: home sweet finally wait going ready work watching wanna airport bus bro im ill walking away lga need jfk laguardia\n",
      "Topic #12: nj jobcircle city jersey jobs hiring com http services developer information princeton east analyst newark mitchell martin manager data union\n",
      "Topic #13: center barclays jay izod rush concert lincoln arts pic nj chelsea world missing instagr vs javits jacob convention going bring\n",
      "Topic #14: good food thing doing life did bad pretty today feels think isn does people look lmao looking things stuff god\n",
      "Topic #15: got today ain people tomorrow real shit lmaoo bitches bad hit niggas man hell ur damn talk let crazy class\n",
      "Topic #16: stadium yankees yankee game lets vs boston let red bronx season tonight yanks win fan october san pic st watch\n",
      "Topic #17: nyc newyork party times hotel square today weekend fall cool coming cafe thanks live girls art soon hey met yeah\n",
      "Topic #18: rt lmao best wanna video wit life http new cause halloween need right thanks ly day getting nigga tweet gym\n",
      "Topic #19: night tonight gonna tomorrow friday date come games college long fun saturday park yankees dead great going sunday debate fall\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### after sandy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 20\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected string or buffer\n",
      "Error tokenizing data. C error: Expected 10 fields in line 445, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 585, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 753, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 677, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 808, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 196, saw 12\n",
      "\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "Error tokenizing data. C error: Expected 10 fields in line 418, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 385, saw 11\n",
      "\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "Error tokenizing data. C error: Expected 10 fields in line 146, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 460, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 224, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 817, saw 11\n",
      "\n",
      "expected string or buffer\n",
      "Error tokenizing data. C error: Expected 10 fields in line 151, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 466, saw 11\n",
      "\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "Error tokenizing data. C error: Expected 10 fields in line 142, saw 11\n",
      "\n",
      "expected string or buffer\n"
     ]
    }
   ],
   "source": [
    "pattern = \"http\"\n",
    "dirs = glob.glob(\"data/data_New_York_City/2012-11*.csv\") + glob.glob(\"data/data_New_York_City/2012-10-3*.csv\")\n",
    "sentences = []\n",
    "for dir_ in dirs:\n",
    "    try:\n",
    "        df = pd.read_csv(dir_, delimiter=\";\")\n",
    "\n",
    "        new_sentences = list(df['text'].values)\n",
    "        for sentence in new_sentences:\n",
    "            sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "            sentence = re.sub(\" \\d+\", '', sentence)\n",
    "            sentence = re.sub(r'\\w*\\d\\w*', '', sentence)\n",
    "            sentence = re.sub('http[s]?://(www. )?(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', sentence)\n",
    "            sentences.append(sentence)\n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: http instagr center photo com rockefeller posted bar st sandy house dinner store christmas tree hall building red day little\n",
      "Topic #1: pic com twitter path http sandy true look bad line finally ready fun yes tree storm happy way street rock\n",
      "Topic #2: new york ny city club jersey bar square manhattan cafe st times terminal restaurant hall station amc bridge broadway flight\n",
      "Topic #3: lol ok oh suck did rt cool hate gonna big fun right man phone ass omg ur better thank happy\n",
      "Topic #4: just foursquare mayor posted photo ousted saw want life took did im ve live week finished asked day fuck half\n",
      "Topic #5: like look looks feel bitch girls shit say bitches eating sure im house sound watching niggas sounds wtf does better\n",
      "Topic #6: don know think come tell going let want mean gonna bad ll make fuck wait wanna use oh son understand\n",
      "Topic #7: love baby thanks real_liam_payne miss make ll little forever follow happy let feel birthday gonna use america girl boy person\n",
      "Topic #8: que la en el mi por se tu te es yo si mas pero lo ya para al le esta\n",
      "Topic #9: good looking bad man doing feels thing mad sounds feel looks food thats movie luck idea pretty know house boy\n",
      "Topic #10: time christmas game year working great wrong bed work dinner come broadway life family right cold ve week dont maybe\n",
      "Topic #11: brooklyn center nets barclays ny vs knicks rockefeller bridge heights brooklynnets game tree basketball tavern house queens tonight pic los\n",
      "Topic #12: home finally sweet wanna stuck away house family light long going haha coming bar yea power way work day terminal\n",
      "Topic #13: people rt shit hate fucking say fuck real dont vote power fake way follow world open obama old talk phone\n",
      "Topic #14: nyc city christmas en queens amazing marathon club traffic sandy music lights broadway nice newyork times station winter thanks hurricanesandy\n",
      "Topic #15: got lmao right power oh ve today tomorrow day phone ass work long money man feel pictures going won yea\n",
      "Topic #16: night tonight going tomorrow saturday date friday game beautiful movie school ready class went guess knicks live dinner getting day\n",
      "Topic #17: really need want ass stop wow man life money guess sex rt wanna think work text christmas bitch day didn\n",
      "Topic #18: ly bit http nj city newyork check sandy rt com post event north deal jersey jobs newark live romney hurricane\n",
      "Topic #19: park square central madison nj garden times union hotel bronx west ny grand washington justin instagr ave east heat fitness\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boston bombing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error tokenizing data. C error: Expected 10 fields in line 275, saw 11\n",
      "\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "Error tokenizing data. C error: Expected 10 fields in line 248, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 946, saw 13\n",
      "\n",
      "expected string or buffer\n",
      "Error tokenizing data. C error: Expected 10 fields in line 29, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 24, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 410, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 1604, saw 11\n",
      "\n",
      "expected string or buffer\n",
      "Error tokenizing data. C error: Expected 10 fields in line 189, saw 11\n",
      "\n",
      "expected string or buffer\n",
      "Error tokenizing data. C error: Expected 10 fields in line 671, saw 11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pattern = \"http\"\n",
    "dirs = glob.glob(\"data/data_Boston/2013-03*.csv\") + glob.glob(\"data/data_Boston/2013-04-0*.csv\")\n",
    "sentences = []\n",
    "for dir_ in dirs:\n",
    "    try:\n",
    "        df = pd.read_csv(dir_, delimiter=\";\")\n",
    "\n",
    "        new_sentences = list(df['text'].values)\n",
    "        for sentence in new_sentences:\n",
    "            sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "            sentence = re.sub(\" \\d+\", '', sentence)\n",
    "            sentence = re.sub(r'\\w*\\d\\w*', '', sentence)\n",
    "            sentence = re.sub('http[s]?://(www. )?(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', sentence)\n",
    "            sentences.append(sentence)\n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: http instagram com dinner photo house td beer day garden st east center dlvr happy little thanks untp bruins square\n",
      "Topic #1: twitter pic com paxeast great lot today best did pax lmfao new hey bruins tonight wish lmao yes game restaurant\n",
      "Topic #2: just got think lmao damn said did haha way home going cuz watch want don today new girl time long\n",
      "Topic #3: like feel people shit ass looks really week rt come seriously ll girls bad live cause makes better look gets\n",
      "Topic #4: boston ma time ly bit house http center east excited school best cambridge way restaurant paxeast college night report bad\n",
      "Topic #5: know don let day wanna shit want doesn thanks mt dont time say yeah tomorrow trying probably ve won start\n",
      "Topic #6: good time going day tonight look im gonna looking yeah bad think looks lmao ve today far ill right pretty\n",
      "Topic #7: lol got right bro shit okay funny didn bitch ya lot come worst friends actually oh high house new team\n",
      "Topic #8: love people better watching night hey best dont hate cause ve big new ll make thanks going time music pic\n",
      "Topic #9: need new fuck damn make life oh stop fucking wait house yeah don cause birthday bad seriously start friends haha\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### boston after "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 15\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected string or buffer\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "Error tokenizing data. C error: Expected 10 fields in line 233, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 43, saw 11\n",
      "\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "Error tokenizing data. C error: Expected 10 fields in line 787, saw 11\n",
      "\n",
      "Error tokenizing data. C error: Expected 10 fields in line 392, saw 11\n",
      "\n",
      "expected string or buffer\n",
      "expected string or buffer\n"
     ]
    }
   ],
   "source": [
    "pattern = \"http\"\n",
    "dirs = glob.glob(\"data/data_Boston/2013-04-15.csv\") + glob.glob(\"data/data_Boston/2013-04-16.csv\") + glob.glob(\"data/data_Boston/2013-04-17.csv\")+ glob.glob(\"data/data_Boston/2013-04-18.csv\")+ glob.glob(\"data/data_Boston/2013-04-19.csv\")+ glob.glob(\"data/data_Boston/2013-04-2*.csv\")\n",
    "sentences = []\n",
    "for dir_ in dirs:\n",
    "    try:\n",
    "        df = pd.read_csv(dir_, delimiter=\";\")\n",
    "\n",
    "        new_sentences = list(df['text'].values)\n",
    "        for sentence in new_sentences:\n",
    "            sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "            sentence = re.sub(\" \\d+\", '', sentence)\n",
    "            sentence = re.sub(r'\\w*\\d\\w*', '', sentence)\n",
    "            sentence = re.sub('http[s]?://(www. )?(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', sentence)\n",
    "            sentences.append(sentence)\n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: http instagram com bostonstrong beautiful redsox park yq fenway new prayforboston photo food day bar sunset square police dinner boston\n",
      "Topic #1: pic twitter com bostonstrong little today game im happy rt gonna prayforboston home bruins night yes tonight school best seen\n",
      "Topic #2: boston ma marathon house bostonstrong city manhunt way vs watertown bar great grill la police sports http bruins market center\n",
      "Topic #3: like feel looks really make guy shit dont nigga stop wait yes wanna prayforboston haha lmao old cause school away\n",
      "Topic #4: fenway park sox red redsox mlb astros vs night houston game lets pic instagram sunset new watching home bostonstrong best\n",
      "Topic #5: just news saw did said life didn friend posted photo shit walk want gonna watching need heard work getting yes\n",
      "Topic #6: love friends life miss man baby really girl news beautiful city great weird ya thank alive wish god let oh\n",
      "Topic #7: don know shit want fuck wanna thingsthatirritateme let food haha tell rt alive trying new miss say http watching hate\n",
      "Topic #8: good day today thanks way great oh bad ll happy big better ready know place twitter tomorrow god long dinner\n",
      "Topic #9: time shit having dinner today favorite fucking tell mom friends away night lmao city year want girl make watching way\n",
      "Topic #10: people think life tell pretty weird need friends open know thing new kid making way day thingsthatirritateme make said hours\n",
      "Topic #11: got did phone guys say wait work report home little police new way text literally ill feel mom nigga market\n",
      "Topic #12: going tonight watch trying make open rt lets birthday nice shots year today phone favorite tv cambridge city person great\n",
      "Topic #13: lol really think girl mad dont fuck need thing text little fun bitch club party walk watching wish shit did\n",
      "Topic #14: right work man said person literally heart amazing oh doing bad scanner wrong police ill ya starbucks fuck restaurant having\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
