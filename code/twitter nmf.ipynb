{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import make_moons\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 20\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###vegas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected string or buffer\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "Error tokenizing data. C error: Expected 10 fields in line 296, saw 11\n",
      "\n",
      "expected string or buffer\n",
      "Error tokenizing data. C error: Expected 10 fields in line 239, saw 11\n",
      "\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "Error tokenizing data. C error: Expected 10 fields in line 1755, saw 11\n",
      "\n",
      "expected string or buffer\n",
      "expected string or buffer\n"
     ]
    }
   ],
   "source": [
    "pattern = \"http\"\n",
    "dirs = glob.glob(\"/Users/krista/Desktop/w266-project-master/data/data_Las_Vegas/*.csv\")\n",
    "sentences = []\n",
    "for dir_ in dirs:\n",
    "    try:\n",
    "        df = pd.read_csv(dir_, delimiter=\";\")\n",
    "#         df = df[~df.text.str.contains(pattern)]\n",
    "        new_sentences = list(df['text'].values)\n",
    "        for sentence in new_sentences:\n",
    "#             regex = re.compile('[^a-zA-Z]')\n",
    "#             sentence = regex.sub(sentence, regex)\n",
    "            sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "            sentence = re.sub(\" \\d+\", '', sentence)\n",
    "            sentence = re.sub(r'\\w*\\d\\w*', '', sentence)\n",
    "\n",
    "            sentences.append(sentence)\n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: vegas las bellagio venetian north en cosmopolitan wynn grand downtown mgm fabulous welcome center palazzo bz good sign convention stratosphere\n",
      "Topic #1: bit ly vegastraffic xxmewb accident http blvd rd clark ave nb sb reported approaching right eb dr wb ramp charleston\n",
      "Topic #2: swarmapp nv henderson las lasairport mccarran international airport lunch bar home cafe center pic grill buffet burger restaurant passenger picking\n",
      "Topic #3: beer untp drinking http photo ipa khourysfinewine ale tenayacreek hopnutsbrewing bangerbrewing zombies bottle share beerhaus lager stout hop light sincitybeer\n",
      "Topic #4: lasvegas en lasvegasstrip bellagio tour usa home sincity hiring wynn circlepix vivalasvegas fremont travel lifeisbeautiful vegas lv mandalaybay vegasstrip brunch\n",
      "Topic #5: casino hotel resort rock mirage aria mandalay bay hard luxor paris planet hollywood spa red york flamingo orleans excalibur en\n",
      "Topic #6: just posted photo video bz got like listed retweet park restaurant world want club nightclub bar red home square circlepix\n",
      "Topic #7: nevada north las paradise henderson enterprise spring valley summerlin south bz morning sunrise dog good lol like home mcdonalds work\n",
      "Topic #8: twitter pic nfl raidernation mp http raiders tour home circlepix week status oakland retweet listed vs dlvr listing virtual newest\n",
      "Topic #9: strip las vegas vegasbaby lasvegasstrip scenery en vegasstrong fun cafe walking lv like pie taking enjoying got vegasstrip want need\n",
      "Topic #10: beautiful life lifeisbeautiful festival lib amazing art friends weekend live people living things music come like hard truly city thank\n",
      "Topic #11: love thank people amazing city place support bestoftheday hate friends beatles instacool instago man need thanks music gotta bz home\n",
      "Topic #12: tonight party nightclub come lets today inside git hakkasanlv ready weekend join going vip sunny hi forecast lo marqueelv live\n",
      "Topic #13: time great good got like long mccarran year international game spend having amazing friends today home thank fun spending airport\n",
      "Topic #14: day today good great pool little game best make work fun favorite start amazing awesome tomorrow beautiful sunday week like\n",
      "Topic #15: new york check got placed listing hotel sign brand today products fall video old music fresh shirt need realty trying\n",
      "Topic #16: store urban theun necessities online deadstock size sizes starting preowned supreme phone order xl nmd large medium available box target\n",
      "Topic #17: vegasstrong arena mobile bz mgm grand vegasgoldenknights garden game welcome prayforvegas knights fabulous hockey goldenknights genblue sign city mandalay bay\n",
      "Topic #18: happy birthday halloween best weekend thank wishing shout celebrating little friend monday safe happyhalloween friday costume town month anniversary girl\n",
      "Topic #19: night great fun saturday friday amazing tomorrow thanks fight ready repost dinner thursday concert got vegas good bz little friends\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###houston before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected string or buffer\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "Error tokenizing data. C error: Expected 10 fields in line 70, saw 11\n",
      "\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "Error tokenizing data. C error: Expected 10 fields in line 489, saw 11\n",
      "\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "expected string or buffer\n",
      "expected string or buffer\n"
     ]
    }
   ],
   "source": [
    "pattern = \"http\"\n",
    "dirs = glob.glob(\"/Users/krista/Desktop/w266-project-master/data/data_Houston/*.csv\")\n",
    "sentences = []\n",
    "for dir_ in dirs:\n",
    "    try:\n",
    "        df = pd.read_csv(dir_, delimiter=\";\")\n",
    "#         df = df[~df.text.str.contains(pattern)]\n",
    "        new_sentences = list(df['text'].values)\n",
    "        for sentence in new_sentences:\n",
    "#             regex = re.compile('[^a-zA-Z]')\n",
    "#             sentence = regex.sub(sentence, regex)\n",
    "            sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "            sentence = re.sub(\" \\d+\", '', sentence)\n",
    "            sentence = re.sub(r'\\w*\\d\\w*', '', sentence)\n",
    "\n",
    "            sentences.append(sentence)\n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: traffic stop delay mins fwy accident cleared outbound lp inbound hwy sw katy tollway rd sam westside northside stall eb\n",
      "Topic #1: houston downtown sam southeast ward houstonstrong en htown northeast htx tollway museum university tx arts center great city people food\n",
      "Topic #2: bubly http beds baths tx dr pearland st ln bath pasadena ct rd porte fresno la deer way blvd bellaire\n",
      "Topic #3: beer untp drinking http photo ipa ale brewing saintarnold conservatoryhtx brashbeer hop buffbrew wfmbrewing petrol_station pub realalebrewing karbachbrewing brash saucer\n",
      "Topic #4: swarmapp tx pearland pasadena houston george grill intercontinental airport bush restaurant land center bar sugar club kitchen pizza cafe kroger\n",
      "Topic #5: twitter pic tour home circlepix dlvr http astros status realestate rt listing virtual listed drive retweet newest listings tx tatus\n",
      "Topic #6: just posted photo video followthesmell fitness stadium listed like got don park school nrg retweet live meyerland alief galleria club\n",
      "Topic #7: today got come great happy birthday thanks did lunch friday going thank work came beautiful week service book awesome hour\n",
      "Topic #8: park minute maid astros game mlb baseball deer champs earnit bayou vs division al buffalo west dlvr hermann good gostros\n",
      "Topic #9: repost get_repost help thank join el htx people music humble asking bio world using link tonight night regrann come list\n",
      "Topic #10: harvey hurricane relief houstonstrong help bubly aftermath affected victims http nrg needs weather irma rain storm center wake story got\n",
      "Topic #11: tonight storm chance pm hi lo forecast augth sugar land come tx sharpstown cloudy night partly saturday patchy fog sun\n",
      "Topic #12: bit ly http traffic water high lanes main affecting sb hwy fwy wb lp nb baytown eb downtown flooding sam\n",
      "Topic #13: day happy labor beautiful great leg school pilates best week fun awesome family tomorrow amazing night summer fitness weekend party\n",
      "Topic #14: texas houston westside north pearland lol southeast town missouri pasadena city happy prayforhouston work eastside god en share mood south\n",
      "Topic #15: love thanks fun travel life thank know baby proud man clients people sweet big good city come home crazy town\n",
      "Topic #16: time great fun like game having work night food amazing trying coming stadium dinner make friend good favorite don long\n",
      "Topic #17: right blocks lane shoulder accident lanes left stop fwy inbound sam blocked stall pkwy lp northside vehicle eastside outbound tollway\n",
      "Topic #18: new got coming bio music link year video shades beat alert arrival soon blog york friend menu week heights single\n",
      "Topic #19: hurricaneharvey prayforhouston safe houstonstrong need stay houstonflood flooded htx water houstontx rain place prayers ready help contact relief downtown victims\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###puerto rico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected string or buffer\n",
      "Error tokenizing data. C error: Expected 10 fields in line 13, saw 11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pattern = \"http\"\n",
    "dirs = glob.glob(\"/Users/krista/Desktop/w266-project-master/data/data_San_Juan/*.csv\")\n",
    "sentences = []\n",
    "for dir_ in dirs:\n",
    "    try:\n",
    "        df = pd.read_csv(dir_, delimiter=\";\")\n",
    "#         df = df[~df.text.str.contains(pattern)]\n",
    "        new_sentences = list(df['text'].values)\n",
    "        for sentence in new_sentences:\n",
    "#             regex = re.compile('[^a-zA-Z]')\n",
    "#             sentence = regex.sub(sentence, regex)\n",
    "            sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "            sentence = re.sub(\" \\d+\", '', sentence)\n",
    "            sentence = re.sub(r'\\w*\\d\\w*', '', sentence)\n",
    "\n",
    "            sentences.append(sentence)\n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: argentina juan san ullun santa lucia lucianopereyra zonda marquesado pocito chimbas albardon villa lavidaalviento todo swarmapp hoy manana hay esta\n",
      "Topic #1: que lo nunca todo lindo siempre ya mejor estoy hay cosas tengo sos hacer hace al solo bien alguien ser\n",
      "Topic #2: la primavera vida hoy si siempre como ya para yo semana mejor tarde nos san bien todo negra desde mayo\n",
      "Topic #3: el para hoy al primer corazon zonda desde nos tu quiero vamos que pero su querido fue nuestra mundo gran\n",
      "Topic #4: mi vida para cumple amor vos sos todo rivadavia cielo amo como hermoso dios ya tu yo hermano alguien vez\n",
      "Topic #5: del bicentenario teatro estadio juan san dia centro sol mundo sala hotel mejor felicidad noche estoy desde rivadavia river todo\n",
      "Topic #6: te amo quiero siempre mucho tanto cuando si felices amor alguien voy algo nunca amigo hasta mejor tarde estar donde\n",
      "Topic #7: por gracias esta su estar siempre juntos lo gran dios mucho vez al tan una ustedes noche si todo hermosa\n",
      "Topic #8: se viene cuando hermano asi tu va donde vez que si seguimos felices otra ver lavidaalviento nuestra una cielo estoy\n",
      "Topic #9: pic twitter swarmapp vs gran todos anos espero como boca ese rivadavia estamos familia amor amigos este hoy juan river\n",
      "Topic #10: los todos nuestra para felices noche son anos fin amigos muy seguimos dias sala quiero viernes que villa la le\n",
      "Topic #11: capital juan san http swarmapp tanto este va domingo primavera otra hasta sin gran vamos para running cumple todas ya\n",
      "Topic #12: una foto publicar acaba villa vez noche negra gran persona hermosa esta hotel vamos juntos lo rivadavia casa tarde muy\n",
      "Topic #13: mute sabado para tus tu anticipadas al vos este primavera seguimos sin esta siempre mucho hasta hermano nos como mis\n",
      "Topic #14: mas nada vos algo para yo hermosa otra dos si una boca cuando chimbas sos juntos vez voy lindo delaostia\n",
      "Topic #15: feliz dia cumpleanos cumple mama para todos mejor todas rivadavia vida colegio mucho muy todo amo mundo primavera amor pablo\n",
      "Topic #16: las hasta todas amo centro desde ya hacer tengo pero son quiero anticipadas seguimos sala cosas juan sin yo noche\n",
      "Topic #17: es como lindo ser esta ver solo tu todo pero tan alguien personas amigo una amigos buen felicidad ustedes le\n",
      "Topic #18: sanjuan boca sol asi argentina sarmiento estadio river viaje centro rivadavia nos casa para bicentenario domingo gracias donde gran cantoni\n",
      "Topic #19: club soul jockey federal_san_juan desamparados noche sportivo quiero san juan lo dia hasta esta mute river todo donde estadio sabado\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
