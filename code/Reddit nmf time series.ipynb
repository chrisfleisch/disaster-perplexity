{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import make_moons\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import datetime\n",
    "import nltk\n",
    "import time\n",
    "import logging\n",
    "import itertools\n",
    "\n",
    "from shared_lib import utils, vocabulary\n",
    "from shared_lib import ngram_lm\n",
    "from shared_lib import ngram_utils\n",
    "from shared_lib import simple_trigram\n",
    "from scipy import sparse\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "        sentences = []\n",
    "        new_sentences = list(df['body'].values)\n",
    "        for sentence in new_sentences:\n",
    "            sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "            sentence = re.sub(\" \\d+\", '', sentence)\n",
    "            sentence = re.sub(r'\\w*\\d\\w*', '', sentence)\n",
    "            sentences.append(sentence)\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_bomb_df = pd.read_json('/Users/krista/Desktop/w266-project-master/data/reddit/boston_comments_2013.03-2013.05.txt', lines=True)\n",
    "boston_series_df = pd.read_json('/Users/krista/Desktop/w266-project-master/data/reddit/boston_comments_2013.10-2013.11.txt', lines=True)\n",
    "colorado_df = pd.read_json('/Users/krista/Desktop/w266-project-master/data/reddit/colorado_comments_2017.06-2017.09.txt', lines=True)\n",
    "florida_df = pd.read_json('/Users/krista/Desktop/w266-project-master/data/reddit/florida_comments_2017.06-2017.09.txt', lines=True)\n",
    "houston_df = pd.read_json('/Users/krista/Desktop/w266-project-master/data/reddit/houston_comments_2017.06-2017.09.txt', lines=True)\n",
    "miami_df = pd.read_json('/Users/krista/Desktop/w266-project-master/data/reddit/miami_comments_2017.06-2017.09.txt', lines=True)\n",
    "nyc_df = pd.read_json('/Users/krista/Desktop/w266-project-master/data/reddit/nyc_comments_2012.08-2012.12.txt', lines=True)\n",
    "puerto_rico_df = pd.read_json('/Users/krista/Desktop/w266-project-master/data/reddit/puerto_rico_comments_2017.06-2017.09.txt', lines=True)\n",
    "vegas_df = pd.read_json('/Users/krista/Desktop/w266-project-master/data/reddit/vegas_comments_2017.06-2017.09.txt', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# setup local times\n",
    "boston_bomb_df['created_at_local'] = pd.to_datetime(boston_bomb_df['created_utc'], unit='s') \\\n",
    "                                     .dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
    "boston_series_df['created_at_local'] = pd.to_datetime(boston_series_df['created_utc'], unit='s') \\\n",
    "                                     .dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
    "florida_df['created_at_local'] = pd.to_datetime(florida_df['created_utc'], unit='s') \\\n",
    "                                     .dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
    "houston_df['created_at_local'] = pd.to_datetime(houston_df['created_utc'], unit='s') \\\n",
    "                                     .dt.tz_localize('UTC').dt.tz_convert('US/Central')\n",
    "miami_df['created_at_local'] = pd.to_datetime(miami_df['created_utc'], unit='s') \\\n",
    "                                     .dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
    "nyc_df['created_at_local'] = pd.to_datetime(nyc_df['created_utc'], unit='s') \\\n",
    "                                 .dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
    "puerto_rico_df['created_at_local'] = pd.to_datetime(puerto_rico_df['created_utc'], unit='s') \\\n",
    "                                 .dt.tz_localize('UTC').dt.tz_convert('America/Puerto_Rico')\n",
    "vegas_df['created_at_local'] = pd.to_datetime(vegas_df['created_utc'], unit='s') \\\n",
    "                                 .dt.tz_localize('UTC').dt.tz_convert('US/Pacific')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2017-05-31 17:02:09-07:00\n",
       "1       2017-05-31 17:25:40-07:00\n",
       "2       2017-05-31 17:34:44-07:00\n",
       "3       2017-05-31 17:38:33-07:00\n",
       "4       2017-05-31 18:17:50-07:00\n",
       "5       2017-05-31 18:34:51-07:00\n",
       "6       2017-05-31 18:38:49-07:00\n",
       "7       2017-05-31 18:49:23-07:00\n",
       "8       2017-05-31 19:03:11-07:00\n",
       "9       2017-05-31 19:04:52-07:00\n",
       "10      2017-05-31 19:06:20-07:00\n",
       "11      2017-05-31 19:10:10-07:00\n",
       "12      2017-05-31 19:12:37-07:00\n",
       "13      2017-05-31 19:22:35-07:00\n",
       "14      2017-05-31 19:23:16-07:00\n",
       "15      2017-05-31 19:25:15-07:00\n",
       "16      2017-05-31 19:27:52-07:00\n",
       "17      2017-05-31 19:36:15-07:00\n",
       "18      2017-05-31 19:37:57-07:00\n",
       "19      2017-05-31 19:45:04-07:00\n",
       "20      2017-05-31 19:52:39-07:00\n",
       "21      2017-05-31 20:17:16-07:00\n",
       "22      2017-05-31 20:38:52-07:00\n",
       "23      2017-05-31 20:39:44-07:00\n",
       "24      2017-05-31 20:40:17-07:00\n",
       "25      2017-05-31 20:42:48-07:00\n",
       "26      2017-05-31 20:45:08-07:00\n",
       "27      2017-05-31 20:45:30-07:00\n",
       "28      2017-05-31 20:45:44-07:00\n",
       "29      2017-05-31 20:46:59-07:00\n",
       "                   ...           \n",
       "17808   2017-09-29 23:54:13-07:00\n",
       "17809   2017-09-30 01:16:30-07:00\n",
       "17810   2017-09-30 01:37:38-07:00\n",
       "17811   2017-09-30 02:01:23-07:00\n",
       "17812   2017-09-30 05:40:03-07:00\n",
       "17813   2017-09-30 07:04:15-07:00\n",
       "17814   2017-09-30 07:24:50-07:00\n",
       "17815   2017-09-30 08:07:29-07:00\n",
       "17816   2017-09-30 08:10:33-07:00\n",
       "17817   2017-09-30 08:16:03-07:00\n",
       "17818   2017-09-30 08:32:37-07:00\n",
       "17819   2017-09-30 08:41:39-07:00\n",
       "17820   2017-09-30 09:02:29-07:00\n",
       "17821   2017-09-30 09:16:21-07:00\n",
       "17822   2017-09-30 09:20:22-07:00\n",
       "17823   2017-09-30 09:42:43-07:00\n",
       "17824   2017-09-30 09:43:49-07:00\n",
       "17825   2017-09-30 10:26:19-07:00\n",
       "17826   2017-09-30 11:40:38-07:00\n",
       "17827   2017-09-30 12:44:37-07:00\n",
       "17828   2017-09-30 13:13:25-07:00\n",
       "17829   2017-09-30 13:53:33-07:00\n",
       "17830   2017-09-30 14:21:09-07:00\n",
       "17831   2017-09-30 14:29:21-07:00\n",
       "17832   2017-09-30 14:35:55-07:00\n",
       "17833   2017-09-30 14:39:02-07:00\n",
       "17834   2017-09-30 15:12:18-07:00\n",
       "17835   2017-09-30 15:16:20-07:00\n",
       "17836   2017-09-30 15:46:39-07:00\n",
       "17837   2017-09-30 16:13:52-07:00\n",
       "Name: created_at_local, Length: 17838, dtype: datetime64[ns, US/Pacific]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_boston = (boston_bomb_df['created_at_local'] > '2013-02-28') & (boston_bomb_df['created_at_local'] <= '2013-04-14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "before_florida = (florida_df['created_at_local'] > '2017-05-31') & (florida_df['created_at_local'] <= '2017-08-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_houston = (houston_df['created_at_local'] > ' 2017-05-31') & (houston_df['created_at_local'] <= '2017-08-16')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "before_miami = (miami_df['created_at_local'] > '2017-05-31') & (miami_df['created_at_local'] <= '2017-08-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "before_nyc = (nyc_df['created_at_local'] > '2012-07-31') & (nyc_df['created_at_local'] <= '2012-10-23')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "before_puerto_rico = (puerto_rico_df['created_at_local'] > '2017-05-31') & (puerto_rico_df['created_at_local'] <= '2017-09-15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_vegas = (vegas_df['created_at_local'] > '2017-05-31') & (vegas_df['created_at_local'] <= '2017-09-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boston shooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 20\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in boston before bombing NMF model:\n",
      "Topic #0: going work home getting make things way probably run high able won school having doesn traffic better different lot time\n",
      "Topic #1: http com www org reddit jpg boston imgur html watch comments link wikipedia https youtube en wiki news amp facebook\n",
      "Topic #2: thanks awesome thank check definitely cool ll looking wow look help haha ah god interesting advice info idea interested oh\n",
      "Topic #3: boston city area cambridge new east places nice live nyc expensive water bars south town cities pizza york check living\n",
      "Topic #4: like looks sounds feel shit old band yea look sound makes stuff kind real crazy idea nah kinda said piece\n",
      "Topic #5: don know want boston let need cab people understand uber sorry care cabs forget internet regulations question anymore drivers seriously\n",
      "Topic #6: just post thought doesn article comment saying mean person guess wanted read wasn globe didn come boston makes say op\n",
      "Topic #7: good food beer luck house pretty just best places bar lol deal delicious know favorite amazing burger cheap seafood restaurant\n",
      "Topic #8: people think point money makes true trying job agree idea making life jobs better hard don isn opinion shitty make\n",
      "Topic #9: street park square north walk end st parking near live ave station somerville harvard cambridge south hill brighton allston cheap\n",
      "Topic #10: ve great years place got new seen ago heard haven best couple think went taken lived gone looking times used\n",
      "Topic #11: think didn said got actually saw did wrong today thought took ticket thinking snow read car say doing probably friend\n",
      "Topic #12: amp great try nice better fun ll probably town old google southie did summer amazing ya museum welcome tour college\n",
      "Topic #13: ll pay day car rent maybe tax service need try come want tell paying taxes month time ask fine cost\n",
      "Topic #14: sure pretty make damn buy ma police cops sweet sell happen days easy im word going union live bro massachusetts\n",
      "Topic #15: line people red green train bus lot orange blue ride living talking station ve street look bet cars sucks downtown\n",
      "Topic #16: yeah love man fuck oh shit night day picture dude wait wish hate kind accent hell post thing late red\n",
      "Topic #17: time did yes card actually used use credit exactly work stolen read story worked blanchard blanchards cards number remember report\n",
      "Topic #18: really city gt does year big bad isn public state mean say building doesn going nice bike dig job old\n",
      "Topic #19: right way guy thing fucking stop believe driving left driver cab day drunk hit drivers work cars uber road totally\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(clean_data(boston_bomb_df.loc[before_boston]))\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"\\nTopics in boston before bombing NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### boston bombing after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(clean_data(boston_bomb_df.loc[-before_boston]))\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in Boston after bombing NMF model:\n",
      "Topic #0: use time long year pay work having money way public days won school car high used need getting make able\n",
      "Topic #1: com http www amp imgur jpg boston watch org https twitter youtube comments html link video google en wikipedia page\n",
      "Topic #2: like looks look feel doesn sounds kind maybe sound bag person looked stuff bomb better dick way pressure different does\n",
      "Topic #3: boston city new like beer town area year strong big favorite best moved cities sox red lived week beautiful living\n",
      "Topic #4: ll work check definitely day good try place better time amazing damn soon ok glad doing awesome friend cool hopefully\n",
      "Topic #5: just went ago saw away called happened hours got came minutes phone edit took explosion morning line said couple watching\n",
      "Topic #6: don thanks know let link ll boston info want exactly appreciate check ok forget understand posting worry need answer sharing\n",
      "Topic #7: people lot help trying stop family working world run blood person things lives need race list running innocent aren tragedy\n",
      "Topic #8: good just people need point don luck idea want boston bad kind donate reason lot care normal strong question charity\n",
      "Topic #9: think don agree point wrong way better makes sense just flair saying going death problem means sign trying life idea\n",
      "Topic #10: right mean street left wrong middle red building stop sure does inside american view obviously hand probably lane disagree talking\n",
      "Topic #11: did police suspect said fbi bomb news saying officer reporting suspects confirmed brother cnn say bombing scanner question custody dead\n",
      "Topic #12: thank great oh awesome yes guys sorry god job love hope say pizza safe today hear stay come okay help\n",
      "Topic #13: guy picture white looking photo hat pictures backpack black wow second man photos pic guys bag taken wearing talking needs\n",
      "Topic #14: really want nice day hope man make going tell great buy mind wouldn fun leave care hate isn wait gets\n",
      "Topic #15: ve heard seen thing got scanner way best haven times read police radio true wish listening listen tv country hear\n",
      "Topic #16: pretty didn know thought actually maybe thing bad say cool things thinking going sure crazy saw isn wasn interesting close\n",
      "Topic #17: reddit post sure gt going thread isn korean make english getting news comment does time real internet read case fake\n",
      "Topic #18: yeah fuck time love shit fucking dude op lol hell seriously got haha life totally week ass say asshole hey\n",
      "Topic #19: live area cambridge place street square st line open walk watertown near house ave night station harvard great park north\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"\\nTopics in Boston after bombing NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Florida Irma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in before Irma in florida NMF model:\n",
      "Topic #0: people work things pay school isn having money doesn insurance cost want income use run instead make high living need\n",
      "Topic #1: com https reddit http www amp comments post np message news originalpostsearcher bot org compose github faq morgan papernotes jpg\n",
      "Topic #2: florida south north water central coast living best panhandle area winter favorite fl summer ocean welcome clear cheap east orlando\n",
      "Topic #3: beach nice area west park orlando key palm city near tampa live hour visit drive beaches fun fl place st\n",
      "Topic #4: like looks sounds cheese feel stuff america different honestly fall imagine driving weed summer water feels neighborhood bit bunch haha\n",
      "Topic #5: just publix bad read fucking heat deal say humidity gun article ya holy edit big probably forgot late wish republicans\n",
      "Topic #6: good man luck idea oh exactly job gets miss guy said thanks kid info know ok rules parents lot company\n",
      "Topic #7: don know need world mean walk miles eat dog want let airport believe stay air answer outside watch ask drive\n",
      "Topic #8: ll thanks didn look probably wait check gonna goes says amazing hell definitely house shot right available need im soon\n",
      "Topic #9: people just thought city pensacola look real expensive needs asking good coming buy lives lot looking food checking gulf sand\n",
      "Topic #10: trump police article wrong public let government right fact trying medical marijuana voted country speed shouldn did law amendment party\n",
      "Topic #11: ve years got seen know ago lived life went getting haven crazy times live sucks moved outside year friends time\n",
      "Topic #12: really think kind cool going agree wow looking hear gainesville great heard wife week knew weeks tampa better unfortunately hey\n",
      "Topic #13: pretty yes love springs sure true sad comment river big picture ve ones old experience shitty false totally kids happy\n",
      "Topic #14: time state think day long beautiful weather aren place wouldn head chance sun rest start step political win interesting safe\n",
      "Topic #15: did sure year make sub rain say doesn guess day damn chicken hot makes days story better came going happen\n",
      "Topic #16: miami thing actually new history sorry buy old level country dude orlando enjoy speak single hours person culture sea building\n",
      "Topic #17: gt lol shit yeah want fuck oh maybe white vote try racist talking black saw id county saying tell law\n",
      "Topic #18: way thank help car stop better ll gt awesome dangerous driving ah morning guys thats scott god question rick access\n",
      "Topic #19: going right great st point does make hope problem best pete tampa end used choice come difference month hands agree\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(clean_data(florida_df.loc[before_florida]))\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"\\nTopics in before Irma in florida NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in during Irma in florida NMF model:\n",
      "Topic #0: probably storm florida new hit state really far hurricanes hard cat time day having better area things remember storms bad\n",
      "Topic #1: com https amp reddit www http comments bot news watch link message np imgur nbsp source org youtube info information\n",
      "Topic #2: power lost went fpl days trees lines night house damage sunday generator tree duke hours ac lucky came grid neighborhood\n",
      "Topic #3: hope bad pretty good luck best tampa orlando little staying eye friend north family hoping friends guys ll fort myers\n",
      "Topic #4: don good know want need idea edit op point run luck care understand able option read help neighbor forget family\n",
      "Topic #5: just maybe fine wait case ago saw wasn turn nope point trying nah like moved ones posted dick wondering plywood\n",
      "Topic #6: like looks look guy sounds looking nice kind stuff times natural plane small kinda absolutely fake bunch try lmao ones\n",
      "Topic #7: storm shelter evacuate evacuation leave windows wind won surge winds zone house flood tell flooding rain away home need worried\n",
      "Topic #8: water food use buy publix store florida price things hot used bottled tap beer ice bottles thing filled bought cold\n",
      "Topic #9: people gt climate aren pets change joke literally saying wish white reason issue black trump stupid bullshit fucking isn understand\n",
      "Topic #10: safe florida stay man better sorry feel place hey glad need ask home hopefully help hope doing heat taking guys\n",
      "Topic #11: know did thanks yes does let great didn doesn actually way cat said help okay exactly ok makes office advice\n",
      "Topic #12: gas work way morning today traffic drive tomorrow friday hours car stations leaving lot getting open road thursday orlando driving\n",
      "Topic #13: think ll sure thanks post definitely guess check gonna start long try miss pretty baby appreciate awesome situation worry super\n",
      "Topic #14: going south miami north florida fucked keys try coming wonder happen im imagine city lake soon supposed broward suck second\n",
      "Topic #15: got ve shit fucking god seen oh sub heard real crazy haha publix time chicken holy floridians damn worse wow\n",
      "Topic #16: hurricane irma days hit year season matthew andrew weeks fun fl jose beer texas party evacuated home did strong shutters\n",
      "Topic #17: thank lol time yeah make sure thing money hell thought say insurance pay wrong believe state job oh needs person\n",
      "Topic #18: right live county beach west coast east st palm mean left pinellas pete key jacksonville area park schools late near\n",
      "Topic #19: fuck really love house gt years come die yeah isn want thinking life dude old year living true lived big\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 20\n",
    "n_top_words = 20\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(clean_data(florida_df.loc[-before_florida]))\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"\\nTopics in during Irma in florida NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### houston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "#                                    max_features=10**5,\n",
    "#                                    stop_words='english',\n",
    "#                                    strip_accents=\"ascii\"\n",
    "#                                   )\n",
    "# tfidf = tfidf_vectorizer.fit_transform(clean_data(houston_df.loc[before_houston]))\n",
    "# tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "# tfidf = tfidf.todense()\n",
    "# tfidf = np.unique(tfidf, axis=0)\n",
    "# tfidf = sparse.csr_matrix(tfidf)\n",
    "# nmf = NMF(n_components=n_components, random_state=1,\n",
    "#           beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "#           l1_ratio=.5).fit(tfidf)\n",
    "# print(\"\\nTopics in before Harvey in Houston NMF model:\")\n",
    "# tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "# print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "#                                    max_features=10**5,\n",
    "#                                    stop_words='english',\n",
    "#                                    strip_accents=\"ascii\"\n",
    "#                                   )\n",
    "# tfidf = tfidf_vectorizer.fit_transform(clean_data(houston_df.loc[-before_houston]))\n",
    "# tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "# tfidf = tfidf.todense()\n",
    "# tfidf = np.unique(tfidf, axis=0)\n",
    "# tfidf = sparse.csr_matrix(tfidf)\n",
    "# nmf = NMF(n_components=n_components, random_state=1,\n",
    "#           beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "#           l1_ratio=.5).fit(tfidf)\n",
    "# print(\"\\nTopics in during/after Harvey in Houston NMF model:\")\n",
    "# tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "# print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Miami Irma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in Miami  before Irma NMF model:\n",
      "Topic #0: people need new problem going lot public start year want things city really bad especially far way isn doesn parking\n",
      "Topic #1: miami beach north city dade best area la lauderdale season fc broward que stadium county pretty places clubs gonna fl\n",
      "Topic #2: com https www http amp reddit google watch news pm message comments link facebook events post imgur org bot info\n",
      "Topic #3: good nice bar cool pretty definitely luck little places happy food cheap restaurant stuff key recommend really havana spot fish\n",
      "Topic #4: like looks look days feel true club hour super kid doral room range taken busy inside amazon price does apple\n",
      "Topic #5: don know like sounds think understand interesting forget word kids seriously said says joke reason dude care miami tried sound\n",
      "Topic #6: beach south park area uber lot downtown wynwood brickell street parking want west usually weekend near kendall everglades water night\n",
      "Topic #7: thanks week sorry thank wow ll ask rain make bro worst won cool guys come list hear miss means police\n",
      "Topic #8: just day museum moved big science actually outside don station ago yep worth went gas month looked school weeks called\n",
      "Topic #9: gt florida bad doesn article isn state rail law case life states closed funny federal mean matter lmao read evidence\n",
      "Topic #10: people want just thread dont mean hate post picture talk american vote sub stop drive overtown white stickied aren literally\n",
      "Topic #11: know going didn let does happened saw op today read wouldn sure went change tell gotta im quite honestly thought\n",
      "Topic #12: lol flanigans guess trying ll dude ok wait make hope fucking friend fine help boat dead hot ticket stupid makes\n",
      "Topic #13: great love place night food little game post live homestead view come story brickell bit big shot beer enjoy family\n",
      "Topic #14: ve got seen probably man actually heard awesome old glad agree way couldn times saying better care haha lived best\n",
      "Topic #15: did think thing sure years free dog house different market took kill believe remember ago job gables pretty lost make\n",
      "Topic #16: car use traffic turn driving drive lane time yeah left damn lights bus airport lanes thought rain road right cars\n",
      "Topic #17: time really say ll try cuban hard year history white wish definitely totally fun spend people plenty weekend meetup deal\n",
      "Topic #18: right check maybe yeah best doing try idea ll grove wrong think publix el open place fishing ave play store\n",
      "Topic #19: shit way fuck work money live yes real guy used oh pay jobs spanish high city language english deal god\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 20\n",
    "n_top_words = 20\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(clean_data(miami_df.loc[before_miami]))\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"\\nTopics in Miami  before Irma NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in Miami before  Irma  NMF model:\n",
      "Topic #0: like bad things gonna doesn better won really storm didn having year florida said years big change sure possible worst\n",
      "Topic #1: power fpl lost internet lines came morning trees days working yesterday area ticket street restored weeks went comcast outage neighborhood\n",
      "Topic #2: miami beach south north dade florida county west broward fl east city lakes according news local live palm hit center\n",
      "Topic #3: com https amp www reddit http news comments watch message weather link stop video bot data imgur information channel edit\n",
      "Topic #4: just hours ago guy love trying case went took moved came google doesnt saw law minutes hour account wanted reason\n",
      "Topic #5: good hope best pretty luck idea new far soon hoping hopefully homestead evacuated place sucks wings bro friend neighbor sorry\n",
      "Topic #6: don know need want let help really understand uber work believe dude waiting care doubt paid asking calling welcome flanigans\n",
      "Topic #7: storm hurricane zone windows building winds wind damage cat shutters high evacuation house hit shelter surge flooding andrew floor roof\n",
      "Topic #8: people yes isn price money poor gouging lot aren true evacuate saying white black taking demand making means prices time\n",
      "Topic #9: water like buy doesn use person food looks tap sounds need drink beer bring hot places stuff bags sound clean\n",
      "Topic #10: know work day didn thing week ve days saw service today let told talking plenty tuesday tv went ago fucking\n",
      "Topic #11: got like come night better lucky life years ago word ve don comcast means question worse higher feels internet dog\n",
      "Topic #12: ll thanks make sure try man fine ok sorry update help check look hey cool maybe job free posting ask\n",
      "Topic #13: way area tomorrow great coral wait gables school cutler probably flights app bay flight station late available usually gas store\n",
      "Topic #14: right safe gas stay live brickell traffic ave left orlando andth feel driving leave road drive st line drove north\n",
      "Topic #15: going time probably long leave friday storm saturday morning die thought monday coming airport thursday ride plan later updates thinking\n",
      "Topic #16: shit fuck hurricane irma fucking post florida dude oh andrew actually exactly gt day right thing real hell models stupid\n",
      "Topic #17: think yeah ve really dont im nice lol mean seen flanigans lmao actually thats makes fucked joke guys guy thing\n",
      "Topic #18: thank home lol does car place say city phone doing tell come damn god love called used wow hialeah sub\n",
      "Topic #19: open did kendall house today west said heard close publix closed little haven onth aventura near parking crazy grove park\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 20\n",
    "n_top_words = 20\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(clean_data(miami_df.loc[-before_miami]))\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"\\nTopics in Miami before  Irma  NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sandy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 20\n",
    "n_top_words = 20\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(clean_data(nyc_df.loc[before_nyc]))\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"\\nTopics NYC before Sandy NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 20\n",
    "n_top_words = 20\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(clean_data(nyc_df.loc[-before_nyc]))\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"\\nTopics in NYC after Sandy NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in Puerto Rico before Maria NMF model:\n",
      "Topic #0: que la es el en para se mas del tiene como los por una ser al lo esta gente hay\n",
      "Topic #1: like don people know just really work want lot island think way make ve spanish did time pretty thing better\n",
      "Topic #2: la en del el isla parte estadidad independencia lado mayoria calle nunca casa internet por reggaeton los ya tienda medio\n",
      "Topic #3: com https www http amp reddit youtube message watch jpg org comments np wiki imgur facebook source compose subject gov\n",
      "Topic #4: el en es ha aqui esta ese dia todo este plebiscito video gobierno falta tipo otro nada sub yunque una\n",
      "Topic #5: en que yo hay tambien donde estan una porque ver uno estoy tienen jajaja aqui voy trabajo amazon puedes tengo\n",
      "Topic #6: puerto rico rican ricans government state america statehood independence congress status states economy independent born laws white colony territory tax\n",
      "Topic #7: es eso pero si lo una bueno esa verdad muy bien buena pa ponce jaja tienes idea mas mi pues\n",
      "Topic #8: los si todos te son solo pero se esos por va mas como estan han problemas claro estados dos partidos\n",
      "Topic #9: por te tu si pero yo era bien estoy la mi ok mal le alguien quiero como ahi lo tanto\n",
      "Topic #10: gracias por lo esto le muchas wtf lt dios boricua ni espanol menos google su veo sabia sus agree ese\n",
      "Topic #11: gt think statehood vote isn wrong trump people man lmao doesn independence hate say fucking pnp true party likely plebiscite\n",
      "Topic #12: se luz fue ya hace mi al cuando hoy aca como va agua dia aqui esta jodio dude hora tengo\n",
      "Topic #13: lo que te la mas como todo tu bayamon mejor rio este si dice gusta cabron espero mano siempre pasa\n",
      "Topic #14: san juan area car old new rincon beach town metro visit mayaguez time hotel condado place places arecibo east coast\n",
      "Topic #15: las para son sin sea desde pero ahora al mi cosas como tengo todas mas manana siempre esa una carreteras\n",
      "Topic #16: power just got water pay company shit house time money live went today carolina electricity home away lost said buy\n",
      "Topic #17: lol post fuck link edit wow haha shit mi mofongo fun super op oh bro ay mierda hope nope bueno\n",
      "Topic #18: pr yeah yes tax nice plan talking usa income need long local independent politics option different federal guns economy linux\n",
      "Topic #19: thanks good ll check thank great right know ve maybe safe help looking irma news sounds awesome luck stay guys\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 20\n",
    "n_top_words = 20\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(clean_data(puerto_rico_df.loc[before_puerto_rico]))\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"\\nTopics in Puerto Rico before Maria NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in Puerto Rico after Maria NMF model:\n",
      "Topic #0: que en la el se los es las por pero para si una como hay del ya yo esta lo\n",
      "Topic #1: going people work island things think pr right time money don lot way ve want thing just flights leave went\n",
      "Topic #2: com https reddit amp www bot comments message http np totesmessenger thread vote compose twitter facebook puertorico puertoricoinformation links respect\n",
      "Topic #3: puerto rico ricans government american rican federal state citizens florida states gt white americans country congress relief texas united nation\n",
      "Topic #4: family hope thank hear heard update able ok haven information safe lives news okay friend friends soon reach caguas area\n",
      "Topic #5: que lo es mas love contact tu esto te se place le yo eso uno creo estamos para decir espero\n",
      "Topic #6: la en el gracias esta mi por del informacion guaynabo si muchas baja familia pa casa cabron toa rojo cabo\n",
      "Topic #7: act jones shipping ships ship law goods american waiver understand foreign repeal year port term jobs make cost needed companies\n",
      "Topic #8: people thank like right feel great tell situation fuck start awesome going instead person sounds seriously black true happening care\n",
      "Topic #9: know ll don really let actually want read history like english makes comes number believe t_d google military right stupid\n",
      "Topic #10: like got days live area house gas looks rio fucked fine pretty mean flooding phone flooded piedras places talking sounds\n",
      "Topic #11: thanks info ve ponce facebook people group posted lot mayaguez look information heard seen getting pictures videos video haha zello\n",
      "Topic #12: san juan dont sent try help trying appreciate working mayor service fema problem coming aid youre know sure sebastian houston\n",
      "Topic #13: just shit sorry guy say point did oh sub trying real wow stop isn says calling im wasn dude life\n",
      "Topic #14: pr like yes years true irma working gov non case culebra world look fema good mobile wonder schools away worse\n",
      "Topic #15: gt post maria info little edit internet link op video pm el bicho account thats bro guess bit gone new\n",
      "Topic #16: trump doing fuck fucking president hate exactly nfl response pr way governor talk administration won explain rossello job attention party\n",
      "Topic #17: help need water food send make way supplies aid use island open doesn getting times running fuel donate money problem\n",
      "Topic #18: think don time yeah lol man better good thing water probably really nice damn electricity hopefully say worst big war\n",
      "Topic #19: power news said hurricane island good bad didn sure does solar storm far gt government damage flooding roads pretty hit\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 20\n",
    "n_top_words = 20\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(clean_data(puerto_rico_df.loc[-before_puerto_rico]))\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"\\nTopics in Puerto Rico after Maria NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vegas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in Vegas before Shooting NMF model:\n",
      "Topic #0: way uber want end try little usually bit home just need away car high maybe hour deal lyft long drive\n",
      "Topic #1: com https www amp http reddit imgur vegas message google try facebook news watch search youtube jpg comments link las\n",
      "Topic #2: thanks check look info definitely ll help man haha wow ok appreciate sidebar gonna today reply link ah awesome love\n",
      "Topic #3: vegas las downtown town north city street fremont live miss trip charleston area east blvd living la park grand old\n",
      "Topic #4: good luck like idea bar food pretty beer recommend best price buffet steak chicken house point decent sounds island fun\n",
      "Topic #5: like looks sounds feel fuck shit doesn op look things california city live kinda sound doing holy nevada man vacation\n",
      "Topic #6: know don need let come care wrong use doing sorry question asking happen kind doesn want friends illegal make answer\n",
      "Topic #7: just read thought post say hope maybe bad good wanted mean internet saw trying new posted god data cox oh\n",
      "Topic #8: people gt bad said fucking cox real life job yes thing school service lot makes world police sense company wasn\n",
      "Topic #9: did work does yes week car long oh dog interested year wait got pm guy hell hours years water hot\n",
      "Topic #10: ll tip people want probably don make money won likely problem tipping service bring minimum tips stop better group believe\n",
      "Topic #11: strip looking dont casino lot people hotel parking places cosmo rooms location far linq fremont casinos budget wynn bars walk\n",
      "Topic #12: great thank place awesome food amazing really pizza agree mind nice best restaurant super spot view love happy eat deal\n",
      "Topic #13: lol pay money shit actually going dude mean youre id legal thats buy question fight im weekend exactly damn state\n",
      "Topic #14: ve got seen nice heard love best used years haven times better old tried work shows lot place hard gone\n",
      "Topic #15: think thing didn ago going month year couple open days friday remember years months happened called saw totally worth didnt\n",
      "Topic #16: time night people watch start going fine talking saw getting coming strip fun safe outside summer walking happens pretty morning\n",
      "Topic #17: really right yeah going area new cool live summerlin drive valley desert red isn south rock stuff close henderson green\n",
      "Topic #18: room hotel mgm stay better ask check rooms book going properties fees hotels resort charge money circus stayed bellagio pool\n",
      "Topic #19: sure free day pretty make tickets club play fun use im buy pool night drink table water drinks games game\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 20\n",
    "n_top_words = 20\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(clean_data(vegas_df.loc[before_vegas]))\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"\\nTopics in Vegas before Shooting NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "After pruning, no terms remain. Try a lower min_df or a higher max_df.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f286751fedf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                                    \u001b[0mstrip_accents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ascii\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                   )\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvegas_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbefore_vegas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtfidf_feature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \"\"\"\n\u001b[0;32m-> 1381\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    888\u001b[0m                                                        \u001b[0mmax_doc_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m                                                        \u001b[0mmin_doc_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m                                                        max_features)\n\u001b[0m\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_limit_features\u001b[0;34m(self, X, vocabulary, high, low, limit)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mkept_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkept_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             raise ValueError(\"After pruning, no terms remain. Try a lower\"\n\u001b[0m\u001b[1;32m    772\u001b[0m                              \" min_df or a higher max_df.\")\n\u001b[1;32m    773\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkept_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremoved_terms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: After pruning, no terms remain. Try a lower min_df or a higher max_df."
     ]
    }
   ],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 20\n",
    "n_top_words = 20\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(clean_data(vegas_df.loc[-before_vegas]))\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"\\nTopics in Vegas before Shooting NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
