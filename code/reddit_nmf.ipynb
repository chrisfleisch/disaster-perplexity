{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "from shared_lib import utils, vocabulary\n",
    "from shared_lib import ngram_lm\n",
    "from shared_lib import ngram_utils\n",
    "from shared_lib import simple_trigram\n",
    "from scipy import sparse\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston_bomb_df = pd.read_json('/Users/krista/Desktop/w266-project-master/data/reddit/boston_comments_2013.03-2013.05.txt', lines=True)\n",
    "boston_series_df = pd.read_json('/Users/krista/Desktop/w266-project-master/data/reddit/boston_comments_2013.10-2013.11.txt', lines=True)\n",
    "colorado_df = pd.read_json('/Users/krista/Desktop/w266-project-master/data/reddit/colorado_comments_2017.06-2017.09.txt', lines=True)\n",
    "florida_df = pd.read_json('/Users/krista/Desktop/w266-project-master/data/reddit/florida_comments_2017.06-2017.09.txt', lines=True)\n",
    "houston_df = pd.read_json('/Users/krista/Desktop/w266-project-master/data/reddit/houston_comments_2017.06-2017.09.txt', lines=True)\n",
    "miami_df = pd.read_json('/Users/krista/Desktop/w266-project-master/data/reddit/miami_comments_2017.06-2017.09.txt', lines=True)\n",
    "nyc_df = pd.read_json('/Users/krista/Desktop/w266-project-master/data/reddit/nyc_comments_2012.08-2012.12.txt', lines=True)\n",
    "puerto_rico_df = pd.read_json('/Users/krista/Desktop/w266-project-master/data/reddit/puerto_rico_comments_2017.06-2017.09.txt', lines=True)\n",
    "vegas_df = pd.read_json('/Users/krista/Desktop/w266-project-master/data/reddit/vegas_comments_2017.06-2017.09.txt', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup local times\n",
    "boston_bomb_df['created_at_local'] = pd.to_datetime(boston_bomb_df['created_utc'], unit='s') \\\n",
    "                                     .dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
    "boston_series_df['created_at_local'] = pd.to_datetime(boston_series_df['created_utc'], unit='s') \\\n",
    "                                     .dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
    "florida_df['created_at_local'] = pd.to_datetime(florida_df['created_utc'], unit='s') \\\n",
    "                                     .dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
    "houston_df['created_at_local'] = pd.to_datetime(houston_df['created_utc'], unit='s') \\\n",
    "                                     .dt.tz_localize('UTC').dt.tz_convert('US/Central')\n",
    "miami_df['created_at_local'] = pd.to_datetime(miami_df['created_utc'], unit='s') \\\n",
    "                                     .dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
    "nyc_df['created_at_local'] = pd.to_datetime(nyc_df['created_utc'], unit='s') \\\n",
    "                                 .dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
    "puerto_rico_df['created_at_local'] = pd.to_datetime(puerto_rico_df['created_utc'], unit='s') \\\n",
    "                                 .dt.tz_localize('UTC').dt.tz_convert('America/Puerto_Rico')\n",
    "vegas_df['created_at_local'] = pd.to_datetime(vegas_df['created_utc'], unit='s') \\\n",
    "                                 .dt.tz_localize('UTC').dt.tz_convert('US/Pacific')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 20\n",
    "n_top_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###vegas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"http\"\n",
    "dirs = glob.glob(\"/Users/krista/Desktop/w266-project-master/data/reddit/vegas_comments_2017.06-2017.09.txt\")\n",
    "sentences = []\n",
    "for dir_ in dirs:\n",
    "    try:\n",
    "        df = pd.read_json(dir_, lines=True)\n",
    "#         df = df[~df.text.str.contains(pattern)]\n",
    "        new_sentences = list(df['body'].values)\n",
    "        for sentence in new_sentences:\n",
    "\n",
    "            sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "            sentence = re.sub(\" \\d+\", '', sentence)\n",
    "            sentence = re.sub(r'\\w*\\d\\w*', '', sentence)\n",
    "\n",
    "            sentences.append(sentence)\n",
    "    except Exception as e:\n",
    "        print e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model:\n",
      "Topic #0: time uber away little just going high getting usually don\n",
      "Topic #1: com https www amp http reddit imgur vegas google watch\n",
      "Topic #2: thanks check look info definitely ll awesome help man ok\n",
      "Topic #3: vegas las downtown city live north area fremont circus town\n",
      "Topic #4: good pretty luck food like idea beer recommend stuff price\n",
      "Topic #5: like looks sounds shit feel fuck look things doesn op\n",
      "Topic #6: know don need let come care wrong question sorry doing\n",
      "Topic #7: just read post say mean good hope saw wanted bad\n",
      "Topic #8: people gt bad said life fucking wasn start school thing\n",
      "Topic #9: does week day long got work oh hours guy time\n",
      "Topic #10: ll tip people money probably don won make tipping tax\n",
      "Topic #11: strip dont lot people casino places rooms street far hotel\n",
      "Topic #12: great thank really place nice love amazing agree food awesome\n",
      "Topic #13: lol room hotel mgm free pay pool dude actually going\n",
      "Topic #14: ve got best years seen heard new used town haven\n",
      "Topic #15: think going didn thing year thought right ago remember old\n",
      "Topic #16: did use work ask home cox doesn house id state\n",
      "Topic #17: yes free time club night tickets bar drinks play day\n",
      "Topic #18: yeah drive really area valley car red rock way desert\n",
      "Topic #19: sure pretty right water make looking im want fun dog\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###florida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = \"http\"\n",
    "dirs = glob.glob('/Users/krista/Desktop/w266-project-master/data/reddit/florida_comments_2017.06-2017.09.txt')\n",
    "sentences = []\n",
    "for dir_ in dirs:\n",
    "    try:\n",
    "        df = pd.read_json(dir_, lines=True)\n",
    "#         df = df[~df.text.str.contains(pattern)]\n",
    "        new_sentences = list(df['body'].values)\n",
    "        for sentence in new_sentences:\n",
    "#             regex = re.compile('[^a-zA-Z]')\n",
    "#             sentence = regex.sub(sentence, regex)\n",
    "            sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "            sentence = re.sub(\" \\d+\", '', sentence)\n",
    "            sentence = re.sub(r'\\w*\\d\\w*', '', sentence)\n",
    "\n",
    "            sentences.append(sentence)\n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in florida NMF model:\n",
      "Topic #0: house bad getting hit went don left outside far goes\n",
      "Topic #1: com https reddit amp www http comments bot news watch\n",
      "Topic #2: florida south state north love central living lived fl coast\n",
      "Topic #3: don people want understand trump fucking money government country care\n",
      "Topic #4: good luck best oh thing idea ok really hear op\n",
      "Topic #5: like looks look looking sounds guy dude fun old shit\n",
      "Topic #6: just maybe case gonna got saw ride late moved guess\n",
      "Topic #7: people going leave evacuate tell evacuation storm shelter problem aren\n",
      "Topic #8: people power fpl lines duke grid company lost solar poor\n",
      "Topic #9: thanks yeah man post doing thought guess trying exactly okay\n",
      "Topic #10: safe stay power hope home sorry friend feel guys glad\n",
      "Topic #11: water need food buy use store beer ice windows generator\n",
      "Topic #12: know did way didn work let said half dont does\n",
      "Topic #13: beach county live area st tampa miami west palm city\n",
      "Topic #14: ve years got time seen sub publix life say read\n",
      "Topic #15: storm doesn does make sure winds mean wind insurance surge\n",
      "Topic #16: sure going pretty gas make car try wait need drive\n",
      "Topic #17: think lol really actually shit point nice right fucked gt\n",
      "Topic #18: ll hurricane irma better probably new fine think orlando likely\n",
      "Topic #19: thank right time fuck yes want day come coast great\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in florida NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###miami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = \"http\"\n",
    "dirs = glob.glob('/Users/krista/Desktop/w266-project-master/data/reddit/miami_comments_2017.06-2017.09.txt')\n",
    "sentences = []\n",
    "for dir_ in dirs:\n",
    "    try:\n",
    "        df = pd.read_json(dir_, lines=True)\n",
    "#         df = df[~df.text.str.contains(pattern)]\n",
    "        new_sentences = list(df['body'].values)\n",
    "        for sentence in new_sentences:\n",
    "#             regex = re.compile('[^a-zA-Z]')\n",
    "#             sentence = regex.sub(sentence, regex)\n",
    "            sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "            sentence = re.sub(\" \\d+\", '', sentence)\n",
    "            sentence = re.sub(r'\\w*\\d\\w*', '', sentence)\n",
    "\n",
    "            sentences.append(sentence)\n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in miami NMF model:\n",
      "Topic #0: just things way family friends school actually car worth bit\n",
      "Topic #1: miami beach south north city dade downtown florida county fl\n",
      "Topic #2: power fpl area lost internet night lines yesterday near hialeah\n",
      "Topic #3: com https amp www reddit http news watch link message\n",
      "Topic #4: good best luck brickell idea food place looking far friend\n",
      "Topic #5: like looks look feel sounds person times guys sound nice\n",
      "Topic #6: don know want need people understand english spanish speak drivers\n",
      "Topic #7: people florida isn money state true poor driving price gt\n",
      "Topic #8: going probably way better getting long time able rain orlando\n",
      "Topic #9: know just people let didn love man dont help guy\n",
      "Topic #10: thanks think don just try ok ll cool update want\n",
      "Topic #11: bad going gt right hope lol say doing said saying\n",
      "Topic #12: hurricane water storm need cat andrew irma windows doesn damage\n",
      "Topic #13: got ve years ago time seen day year week went\n",
      "Topic #14: want water gas right real boat left buy literally drink\n",
      "Topic #15: lol pretty check really little park ll nice great flanigans\n",
      "Topic #16: time right work open sure use traffic car make gas\n",
      "Topic #17: did does think place great thing mean read ave come\n",
      "Topic #18: thank safe ll stay zone west storm fine hit home\n",
      "Topic #19: yeah shit live fuck house oh gables kendall fucking god\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in miami NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###puerto rico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = \"http\"\n",
    "dirs = glob.glob('/Users/krista/Desktop/w266-project-master/data/reddit/puerto_rico_comments_2017.06-2017.09.txt')\n",
    "sentences = []\n",
    "for dir_ in dirs:\n",
    "    try:\n",
    "        df = pd.read_json(dir_, lines=True)\n",
    "#         df = df[~df.text.str.contains(pattern)]\n",
    "        new_sentences = list(df['body'].values)\n",
    "        for sentence in new_sentences:\n",
    "#             regex = re.compile('[^a-zA-Z]')\n",
    "#             sentence = regex.sub(sentence, regex)\n",
    "            sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "            sentence = re.sub(\" \\d+\", '', sentence)\n",
    "            sentence = re.sub(r'\\w*\\d\\w*', '', sentence)\n",
    "\n",
    "            sentences.append(sentence)\n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model:\n",
      "Topic #0: la que el en para es por una del se\n",
      "Topic #1: people pr time need way island going right help make\n",
      "Topic #2: com https www amp http reddit facebook message youtube twitter\n",
      "Topic #3: que lo hay yo tu como es mas le creo\n",
      "Topic #4: puerto rico rican ricans statehood state government independence congress federal\n",
      "Topic #5: el del lol esta video aqui este maria link mobile\n",
      "Topic #6: en yo tengo que mi hay estoy tambien donde estan\n",
      "Topic #7: family thank info hope hear news information heard ok help\n",
      "Topic #8: es eso lo pero si mi bueno pa ponce esa\n",
      "Topic #9: don think really vote reddit read thread bot place care\n",
      "Topic #10: los son pr si todos pero op es esos nope\n",
      "Topic #11: san juan area live island old rio areas near service\n",
      "Topic #12: por te gracias si lo tu esta alguien pero fue\n",
      "Topic #13: know ll let don try work spanish dont want really\n",
      "Topic #14: gt yes man did isn love english year lt leave\n",
      "Topic #15: people like fuck just said fucking feel shit thank im\n",
      "Topic #16: like thanks check sounds post great looks right real cool\n",
      "Topic #17: se las la va luz ve pero sin si fue\n",
      "Topic #18: power just got water good house hurricane bad sure days\n",
      "Topic #19: just act jones trump does american point pr shipping pay\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = \"http\"\n",
    "dirs = glob.glob('/Users/krista/Desktop/w266-project-master/data/reddit/boston_comments_2013.03-2013.05.txt')\n",
    "sentences = []\n",
    "for dir_ in dirs:\n",
    "    try:\n",
    "        df = pd.read_json(dir_, lines=True)\n",
    "#         df = df[~df.text.str.contains(pattern)]\n",
    "        new_sentences = list(df['body'].values)\n",
    "        for sentence in new_sentences:\n",
    "#             regex = re.compile('[^a-zA-Z]')\n",
    "#             sentence = regex.sub(sentence, regex)\n",
    "            sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "            sentence = re.sub(\" \\d+\", '', sentence)\n",
    "            sentence = re.sub(r'\\w*\\d\\w*', '', sentence)\n",
    "\n",
    "            sentences.append(sentence)\n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###houston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pattern = \"http\"\n",
    "# dirs = glob.glob('/Users/krista/Desktop/w266-project-master/data/reddit/houston_comments_2017.06-2017.09.txt')\n",
    "# sentences = []\n",
    "# for dir_ in dirs:\n",
    "#     try:\n",
    "#         df = pd.read_json(dir_, lines=True)\n",
    "# #         df = df[~df.text.str.contains(pattern)]\n",
    "#         new_sentences = list(df['body'].values)\n",
    "#         for sentence in new_sentences:\n",
    "# #             regex = re.compile('[^a-zA-Z]')\n",
    "# #             sentence = regex.sub(sentence, regex)\n",
    "#             sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "#             sentence = re.sub(\" \\d+\", '', sentence)\n",
    "#             sentence = re.sub(r'\\w*\\d\\w*', '', sentence)\n",
    "\n",
    "#             sentences.append(sentence)\n",
    "#     except Exception as e:\n",
    "#         print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "#                                    max_features=10**5,\n",
    "#                                    stop_words='english',\n",
    "#                                    strip_accents=\"ascii\"\n",
    "#                                   )\n",
    "# tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
    "# tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "# tfidf = tfidf.todense()\n",
    "# tfidf = np.unique(tfidf, axis=0)\n",
    "# tfidf = sparse.csr_matrix(tfidf)\n",
    "# print 'tfidf done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nmf = NMF(n_components=n_components, random_state=1,\n",
    "#           beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "#           l1_ratio=.5).fit(tfidf)\n",
    "# print 'nmf done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(\"\\nTopics in NMF model:\")\n",
    "# tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "# print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###nyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = \"http\"\n",
    "dirs = glob.glob('/Users/krista/Desktop/w266-project-master/data/reddit/nyc_comments_2012.08-2012.12.txt')\n",
    "sentences = []\n",
    "for dir_ in dirs:\n",
    "    try:\n",
    "        df = pd.read_json(dir_, lines=True)\n",
    "#         df = df[~df.text.str.contains(pattern)]\n",
    "        new_sentences = list(df['body'].values)\n",
    "        for sentence in new_sentences:\n",
    "#             regex = re.compile('[^a-zA-Z]')\n",
    "#             sentence = regex.sub(sentence, regex)\n",
    "            sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
    "            sentence = re.sub(\" \\d+\", '', sentence)\n",
    "            sentence = re.sub(r'\\w*\\d\\w*', '', sentence)\n",
    "\n",
    "            sentences.append(sentence)\n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=10,\n",
    "                                   max_features=10**5,\n",
    "                                   stop_words='english',\n",
    "                                   strip_accents=\"ascii\"\n",
    "                                  )\n",
    "tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "tfidf = tfidf.todense()\n",
    "tfidf = np.unique(tfidf, axis=0)\n",
    "tfidf = sparse.csr_matrix(tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
